{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 找出 text_1 == text_2 的句子对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:30:09.661882Z",
     "iopub.status.busy": "2024-10-14T02:30:09.661167Z",
     "iopub.status.idle": "2024-10-14T02:30:09.870646Z",
     "shell.execute_reply": "2024-10-14T02:30:09.869359Z",
     "shell.execute_reply.started": "2024-10-14T02:30:09.661832Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The number of lines in work/test_B_processed.txt is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "=============== In Progress ===============\r\n",
      "0.00 of text pairs for different subsets identification have processed \r\n",
      "10.00 of text pairs for different subsets identification have processed \r\n",
      "20.00 of text pairs for different subsets identification have processed \r\n",
      "30.00 of text pairs for different subsets identification have processed \r\n",
      "40.00 of text pairs for different subsets identification have processed \r\n",
      "50.00 of text pairs for different subsets identification have processed \r\n",
      "60.00 of text pairs for different subsets identification have processed \r\n",
      "70.00 of text pairs for different subsets identification have processed \r\n",
      "80.00 of text pairs for different subsets identification have processed \r\n",
      "90.00 of text pairs for different subsets identification have processed \r\n",
      "===============     Done    ===============\r\n",
      "4116\r\n",
      "萧红带我回到身后的村庄的启示\t萧红带我回到身后的村庄的启示\r\n",
      "4116\r\n",
      "13\t1\r\n",
      "4116\r\n",
      "\r\n",
      "one in homophone_text_pairs_1_list accounts for 98.83\r\n",
      "4068\r\n",
      "句子对: 萧红带我回到身后的村庄的启示\t萧红带我回到身后的村庄的启示\t13\t1\r\n",
      "句子对: 发烧可以开空调\t发烧可以开空调\t59\t1\r\n",
      "句子对: 见闻vip怎么样\t见闻vip怎么样\t60\t1\r\n",
      "句子对: 标志杆厂家六千二百六十六一七百八十八沧州胜翔实体厂家电话\t标志杆厂家六千二百六十六一七百八十八沧州胜翔实体厂家电话\t96\t1\r\n",
      "句子对: 小狗活动的特点有哪些\t小狗活动的特点有哪些\t186\t1\r\n",
      "句子对: 兰州汽车东站投诉电话\t兰州汽车东站投诉电话\t201\t1\r\n",
      "句子对: 贵定到金华高铁票查询\t贵定到金华高铁票查询\t241\t1\r\n",
      "句子对: 扎手指验血能验出是否怀孕么\t扎手指验血能验出是否怀孕么\t252\t1\r\n",
      "句子对: 武警一个师有多少人\t武警一个师有多少人\t293\t1\r\n",
      "句子对: 会不会有雨\t会不会有雨\t305\t1\r\n",
      "句子对: 来月经可以吃海鲜\t来月经可以吃海鲜\t361\t1\r\n",
      "句子对: 六十四加六十四\t六十四加六十四\t383\t1\r\n",
      "句子对: 借酒消愁的伤感句子\t借酒消愁的伤感句子\t435\t1\r\n",
      "句子对: 胃疼可以吃水果\t胃疼可以吃水果\t528\t1\r\n",
      "句子对: 中华小子小龙被吸取青春是哪一集\t中华小子小龙被吸取青春是哪一集\t598\t1\r\n",
      "句子对: 手撕牛肉的做法大全\t手撕牛肉的做法大全\t605\t1\r\n",
      "句子对: 有什么好看的电影\t有什么好看的电影\t622\t1\r\n",
      "句子对: 抗过敏药吃了会过敏吗\t抗过敏药吃了会过敏吗\t670\t1\r\n",
      "句子对: 小孩能刮痧\t小孩能刮痧\t693\t1\r\n",
      "句子对: 上航制服款式\t上航制服款式\t715\t1\r\n",
      "句子对: 双重趋避冲突的例子三个\t双重趋避冲突的例子三个\t736\t1\r\n",
      "句子对: 建行app在哪买原油\t建行app在哪买原油\t765\t1\r\n",
      "句子对: 你会吹口哨吗\t你会吹口哨吗\t790\t1\r\n",
      "句子对: 北京五环的房价是多少\t北京五环的房价是多少\t803\t1\r\n",
      "句子对: 希戈艺术学校学费多少\t希戈艺术学校学费多少\t812\t1\r\n",
      "句子对: 打开作业帮\t打开作业帮\t816\t1\r\n",
      "句子对: 海曙可仁大药房电话\t海曙可仁大药房电话\t860\t1\r\n",
      "句子对: 西山区第一中学升学率\t西山区第一中学升学率\t867\t1\r\n",
      "句子对: 一活到老学到老老不服老画亦精字亦精两处处飞花飞处处\t一活到老学到老老不服老画亦精字亦精两处处飞花飞处处\t921\t1\r\n",
      "句子对: 被召唤巨人是什么体验笔趣阁\t被召唤巨人是什么体验笔趣阁\t934\t1\r\n",
      "句子对: 蟑螂会咬人\t蟑螂会咬人\t968\t1\r\n",
      "句子对: 甲辰日怎样才能富贵\t甲辰日怎样才能富贵\t1031\t1\r\n",
      "句子对: 涤纶面料裤子夏天穿会不会热\t涤纶面料裤子夏天穿会不会热\t1065\t1\r\n",
      "句子对: 关掉音乐\t关掉音乐\t1093\t1\r\n",
      "句子对: 华为十点八寸的平板多少厘米\t华为十点八寸的平板多少厘米\t1098\t1\r\n",
      "句子对: 热敷消肿\t热敷消肿\t1110\t1\r\n",
      "句子对: 吗啡片多长时间起效\t吗啡片多长时间起效\t1127\t1\r\n",
      "句子对: 怀孕能吃生蚝\t怀孕能吃生蚝\t1158\t1\r\n",
      "句子对: 明日之后建筑怎么回收\t明日之后建筑怎么回收\t1162\t1\r\n",
      "句子对: 孕期能吃烧烤\t孕期能吃烧烤\t1166\t1\r\n",
      "句子对: 小叶增生可以喝豆浆\t小叶增生可以喝豆浆\t1180\t1\r\n",
      "句子对: 淘宝退货单号总是不正确\t淘宝退货单号总是不正确\t1197\t1\r\n",
      "句子对: excel工作表sheet排序\texcel工作表sheet排序\t1209\t1\r\n",
      "句子对: 如何改wifi密码\t如何改wifi密码\t1214\t1\r\n",
      "句子对: 孕妇可以吃鸭肉\t孕妇可以吃鸭肉\t1234\t1\r\n",
      "句子对: 抖音里面的流行音乐怎么没有搜索到\t抖音里面的流行音乐怎么没有搜索到\t1241\t1\r\n",
      "句子对: 按哪个穴位可以缓解头晕\t按哪个穴位可以缓解头晕\t1286\t1\r\n",
      "句子对: 微波炉能做披萨\t微波炉能做披萨\t1291\t1\r\n",
      "句子对: 高血压能吃黄瓜\t高血压能吃黄瓜\t1296\t1\r\n",
      "句子对: 三加二减五\t三加二减五\t1341\t1\r\n",
      "句子对: 怀孕能吃蚬子\t怀孕能吃蚬子\t1357\t1\r\n",
      "48\r\n",
      "句子对: 你知道是\t你知道是\t1111\t0\r\n",
      "句子对: ar镀膜玻璃的\tar镀膜玻璃的\t1297\t0\r\n",
      "句子对: 我要运动\t我要运动\t2523\t0\r\n",
      "句子对: oppowatch防水\toppowatch防水\t4164\t0\r\n",
      "句子对: 大连大学排名\t大连大学排名\t6908\t0\r\n",
      "句子对: 能组什么词\t能组什么词\t7671\t0\r\n",
      "句子对: 咱们聪明\t咱们聪明\t13375\t0\r\n",
      "句子对: 玩一会儿\t玩一会儿\t15289\t0\r\n",
      "句子对: 能组什么词\t能组什么词\t22085\t0\r\n",
      "句子对: 用英语怎么读\t用英语怎么读\t23080\t0\r\n",
      "句子对: 你没有眼睛\t你没有眼睛\t23769\t0\r\n",
      "句子对: 用英语怎么说\t用英语怎么说\t25038\t0\r\n",
      "句子对: 还没回来\t还没回来\t25984\t0\r\n",
      "句子对: 怎么样的\t怎么样的\t27621\t0\r\n",
      "句子对: 教学设计过程模式\t教学设计过程模式\t28158\t0\r\n",
      "句子对: 你有脑袋\t你有脑袋\t28972\t0\r\n",
      "句子对: 形容珍惜的\t形容珍惜的\t30076\t0\r\n",
      "句子对: 没有网络\t没有网络\t30886\t0\r\n",
      "句子对: oppocare加的退换规则\toppocare加的退换规则\t30900\t0\r\n",
      "句子对: 广播电视播音员主持人资格证\t广播电视播音员主持人资格证\t33034\t0\r\n",
      "句子对: 零除零\t零除零\t39053\t0\r\n",
      "句子对: 需要刷机\t需要刷机\t43146\t0\r\n",
      "句子对: 双麦降噪的\t双麦降噪的\t47383\t0\r\n",
      "句子对: 用日语说\t用日语说\t47496\t0\r\n",
      "句子对: 我叫什么\t我叫什么\t50614\t0\r\n",
      "句子对: 换电话微信怎么恢复\t换电话微信怎么恢复\t51953\t0\r\n",
      "句子对: 农行信用卡怎么提前还款\t农行信用卡怎么提前还款\t59567\t0\r\n",
      "句子对: 了解的英文\t了解的英文\t61715\t0\r\n",
      "句子对: 么打电话\t么打电话\t62865\t0\r\n",
      "句子对: 身份证号码怎么\t身份证号码怎么\t64766\t0\r\n",
      "句子对: 帮我找找\t帮我找找\t65619\t0\r\n",
      "句子对: 一千零八十p高清画质的\t一千零八十p高清画质的\t66241\t0\r\n",
      "句子对: 快手怎么查看别人\t快手怎么查看别人\t66878\t0\r\n",
      "句子对: 日文怎么说\t日文怎么说\t68609\t0\r\n",
      "句子对: 什么都可以\t什么都可以\t68686\t0\r\n",
      "句子对: 五分加五分\t五分加五分\t71517\t0\r\n",
      "句子对: 怀孕能吃火鸡\t怀孕能吃火鸡\t72236\t0\r\n",
      "句子对: 我普通话不标准\t我普通话不标准\t73419\t0\r\n",
      "句子对: 打开快手\t打开快手\t76628\t0\r\n",
      "句子对: 我的心在哪里\t我的心在哪里\t76682\t0\r\n",
      "句子对: 你会长大\t你会长大\t76972\t0\r\n",
      "句子对: 新冠状病毒的症状头痛\t新冠状病毒的症状头痛\t79798\t0\r\n",
      "句子对: 你会吹牛逼\t你会吹牛逼\t80405\t0\r\n",
      "句子对: 你智力好\t你智力好\t90811\t0\r\n",
      "句子对: 成语接龙\t成语接龙\t91684\t0\r\n",
      "句子对: 零除零\t零除零\t95977\t0\r\n",
      "句子对: 会不会学猫叫\t会不会学猫叫\t96223\t0\r\n",
      "句子对: 你有爱人\t你有爱人\t99738\t0\r\n"
     ]
    }
   ],
   "source": [
    "# 查询带有指定字词的句子对\n",
    "def compare_equal_text_pair(text_list_1, text_list_2, text_label_list, print_progress = False):\n",
    "    text_pairs_list = []\n",
    "    label_list = []\n",
    "    print(\"=\"*15 + \" In Progress \" + \"=\"*15)\n",
    "    for i in range (0, len(text_list_1)):\n",
    "        text_1 = text_list_1[i]\n",
    "        text_2 = text_list_2[i]\n",
    "        \n",
    "        if text_1 == text_2:\n",
    "            text_pairs_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "            if text_label_list!= None:\n",
    "                label_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    " \n",
    "        if print_progress and i%10000 == 0:\n",
    "            print(\"%.2f of text pairs for different subsets identification have processed \" %((i/len(text_list_1))*100))    \n",
    "    print(\"=\"*15 + \"     Done    \" + \"=\"*15)\n",
    "    return text_pairs_list, label_list\n",
    "\n",
    "# 使用 准备数据\n",
    "test_text_pair_1_list,test_text_pair_2_list = read_candidates_test(\"work/test_B_processed.txt\")\n",
    "view_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\")\n",
    "\n",
    "# 进行识别\n",
    "equal_text_pairs_list, equal_label_list = compare_equal_text_pair(test_text_pair_1_list,test_text_pair_2_list, view_label_list, True)\n",
    "print(len(equal_text_pairs_list))\n",
    "print(equal_text_pairs_list[0])\n",
    "print(len(equal_label_list))\n",
    "print(equal_label_list[0])\n",
    "write_file(\"common_data/equal_1_list.txt\",equal_text_pairs_list)\n",
    "print(len(read_candidates_line(\"common_data/equal_1_list.txt\")))\n",
    "\n",
    "# 打印结果\n",
    "test_zero_list, test_one_list = summarize_one_zero(equal_text_pairs_list, equal_label_list)\n",
    "print(\"\\none in homophone_text_pairs_1_list accounts for %.2f\" %(len(test_one_list)/len(equal_text_pairs_list)*100))\n",
    "print(len(test_one_list))\n",
    "print_examples(test_one_list, None, None, 50)\n",
    "print(len(test_zero_list))\n",
    "print_examples(test_zero_list, None, None, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 检测仅有同音字不同的句子对（lazypinyin）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:31:02.741439Z",
     "iopub.status.busy": "2024-10-14T02:31:02.740404Z",
     "iopub.status.idle": "2024-10-14T02:31:27.859309Z",
     "shell.execute_reply": "2024-10-14T02:31:27.858330Z",
     "shell.execute_reply.started": "2024-10-14T02:31:02.741397Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The number of lines in work/test_B_processed.txt is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "=============== In Progress ===============\r\n",
      "10.00 of text pairs for different subsets identification have processed \r\n",
      "20.00 of text pairs for different subsets identification have processed \r\n",
      "30.00 of text pairs for different subsets identification have processed \r\n",
      "40.00 of text pairs for different subsets identification have processed \r\n",
      "50.00 of text pairs for different subsets identification have processed \r\n",
      "60.00 of text pairs for different subsets identification have processed \r\n",
      "70.00 of text pairs for different subsets identification have processed \r\n",
      "80.00 of text pairs for different subsets identification have processed \r\n",
      "90.00 of text pairs for different subsets identification have processed \r\n",
      "100.00 of text pairs for different subsets identification have processed \r\n",
      "===============     Done    ===============\r\n",
      "3349\r\n",
      "小孩补什么钙好\t小孩补什么盖好\r\n",
      "3349\r\n",
      "40\t1\r\n",
      "\r\n",
      "one in homophone_text_pairs_1_list accounts for 96.24\r\n",
      "zero in homophone_text_pairs_1_list accounts for 3.76\r\n",
      "3223\r\n",
      "句子对: 小孩补什么钙好\t小孩补什么盖好\t40\t1\r\n",
      "句子对: 单字名字女\t单子名字女\t61\t1\r\n",
      "句子对: 怀孕流褐色血\t怀孕留褐色血\t71\t1\r\n",
      "句子对: 台湾著名大学世界排名\t台湾知名大学世界排名\t82\t1\r\n",
      "句子对: 水浒传作者\t水浒转作者\t166\t1\r\n",
      "句子对: 月经不调吃什么药\t月经不调吃什么要\t171\t1\r\n",
      "句子对: 额头长痘痘是什么原因\t额头张痘痘是什么原因\t192\t1\r\n",
      "句子对: 被猫咬了要打针吗\t背猫咬了要打针吗\t238\t1\r\n",
      "句子对: 猕猴桃能和酸奶一起吃吗\t弥猴桃能和酸奶一起吃吗\t308\t1\r\n",
      "句子对: 月经期可以艾灸吗\t月经期可以爱灸吗\t332\t1\r\n",
      "句子对: 甲醛有气味吗\t甲荃有气味吗\t338\t1\r\n",
      "句子对: 来例假能喝茶吗\t来历假能喝茶吗\t345\t1\r\n",
      "句子对: 腰骶部酸痛怎么回事\t腰底部酸痛怎么回事\t350\t1\r\n",
      "句子对: 心率快吃什么药\t心率块吃什么药\t397\t1\r\n",
      "句子对: 脱发挂什么科\t脱发挂什么课\t449\t1\r\n",
      "句子对: 文言文翻译器\t文言文翻译其\t479\t1\r\n",
      "句子对: 经期能喝茶吗\t经期能喝茶马\t492\t1\r\n",
      "句子对: 哺乳期能贴膏药吗\t哺乳期能帖膏药吗\t513\t1\r\n",
      "句子对: 肺结节是什么\t肺接节是什么\t543\t1\r\n",
      "句子对: 甲状腺结节伴钙化\t甲状腺结界伴钙化\t561\t1\r\n",
      "句子对: 腰肌劳损怎么办\t腰肌劳损怎么半\t594\t1\r\n",
      "句子对: 血糖高的症状\t血糖高德症状\t668\t1\r\n",
      "句子对: 脚后跟疼是什么原因\t脚后跟疼是什么愿因\t702\t1\r\n",
      "句子对: 指甲上有竖纹是怎么回事\t指甲上有竖文是怎么回事\t724\t1\r\n",
      "句子对: 慢性肠胃炎吃什么药\t漫性肠胃炎吃什么药\t751\t1\r\n",
      "句子对: 胃不好能喝红茶吗\t胃不好能和红茶吗\t760\t1\r\n",
      "句子对: 草珊瑚的功效与作用\t草珊湖的功效与作用\t771\t1\r\n",
      "句子对: 肠梗阻怎么治疗\t肠耿阻怎么治疗\t845\t1\r\n",
      "句子对: 哪里有免费电影\t那里有免费电影\t891\t1\r\n",
      "句子对: 一直拉肚子是怎么回事\t一只拉肚子是怎么回事\t892\t1\r\n",
      "句子对: 戴避孕套会怀孕吗\t待避孕套会怀孕吗\t902\t1\r\n",
      "句子对: 橘子吃了有什么好处\t桔子吃了有什么好处\t941\t1\r\n",
      "句子对: 哪种猫好养\t那种猫好养\t945\t1\r\n",
      "句子对: 经期可以吃什么水果\t经期可以吃什麼水果\t957\t1\r\n",
      "句子对: 肾癌的症状\t肾癌得症状\t1012\t1\r\n",
      "句子对: 企业帐户怎么开通\t企业账户怎么开通\t1019\t1\r\n",
      "句子对: 阑尾炎手术后吃什么好\t兰尾炎手术后吃什么好\t1090\t1\r\n",
      "句子对: 被蜜蜂蛰了怎么处理\t被密蜂蛰了怎么处理\t1101\t1\r\n",
      "句子对: 公积金可以取吗\t公基金可以取吗\t1123\t1\r\n",
      "句子对: 心率慢是什么原因\t心律慢是什么原因\t1250\t1\r\n",
      "句子对: 先兆流产的症状\t先找流产的症状\t1256\t1\r\n",
      "句子对: 胰腺是什么\t遗腺是什么\t1269\t1\r\n",
      "句子对: 怎样剪腿上的肉\t怎样减腿上的肉\t1283\t1\r\n",
      "句子对: 甲状腺结节有什么症状\t甲状腺结节有什么证状\t1284\t1\r\n",
      "句子对: 尖锐湿疣如何治疗\t尖锐湿尤如何治疗\t1324\t1\r\n",
      "句子对: 尖锐湿疣传播途径\t尖锐湿疣传播途经\t1361\t1\r\n",
      "句子对: 感冒吃什么药\t敢冒吃什么药\t1457\t1\r\n",
      "句子对: 黄瓜片敷脸有什么好处\t黄瓜片肤脸有什么好处\t1465\t1\r\n",
      "句子对: 梦见被蛇咬\t梦见备蛇咬\t1483\t1\r\n",
      "句子对: 心血管堵塞\t心学管堵塞\t1532\t1\r\n",
      "句子对: 大便出血怎么办\t大便初血怎么办\t1533\t1\r\n",
      "126\r\n",
      "句子对: 山西应急厅厅长\t陕西应急厅厅长\t3306\t0\r\n",
      "句子对: 主动的动字怎么写\t自动的动字怎么写\t8119\t0\r\n",
      "句子对: 冀州在哪个省\t蓟州在哪个省\t21043\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 配件英语\t备件英语\t32868\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 什么的翅膀\t什么的翅胖\t35160\t0\r\n",
      "句子对: 落枕脖子疼怎么办\t洛枕脖子疼怎么办\t39739\t0\r\n",
      "句子对: 癣会传染吗\t显会传染吗\t46709\t0\r\n",
      "句子对: 氯化钠是不是生理盐水\t录化钠是不是生理盐水\t47905\t0\r\n",
      "句子对: 陈字偏旁是什么\t陈字边旁是什么\t59741\t0\r\n",
      "句子对: 陈字偏旁是什么\t陈字边旁是什么\t59741\t0\r\n",
      "句子对: 陈字偏旁是什么\t陈字边旁是什么\t59741\t0\r\n",
      "\r\n",
      "one in homophone_text_pairs_0_list accounts for 38.10\r\n",
      "zero in homophone_text_pairs_0_list accounts for 61.90\r\n",
      "8\r\n",
      "句子对: 章子怡是那里人\t张子怡是那里人\t2284\t1\r\n",
      "句子对: 刘晓燕哪里人\t刘小燕哪里人\t21532\t1\r\n",
      "句子对: 刘恺威多少岁数\t刘凯威多少岁数\t32870\t1\r\n",
      "句子对: 贾玲多少岁了\t嘉玲多少岁了\t37070\t1\r\n",
      "句子对: 王健林哪里人\t王建林哪里人\t49319\t1\r\n",
      "句子对: 韩语姐姐怎么说\t汉语姐姐怎么说\t56723\t1\r\n",
      "句子对: 唐嫣哪里人\t唐岩哪里人\t65984\t1\r\n",
      "句子对: 朱德是那里人\t朱徳是那里人\t66374\t1\r\n",
      "13\r\n",
      "句子对: 例假前十天是安全期吗\t例假前四天是安全期吗\t12631\t0\r\n",
      "句子对: 月初十是什么星座\t月初四是什么星座\t14369\t0\r\n",
      "句子对: 五月十日是什么星座\t五月四日是什么星座\t17082\t0\r\n",
      "句子对: 十和十五用短除法计算\t四和十五用短除法计算\t18796\t0\r\n",
      "句子对: 十除零点四\t四除零点四\t20839\t0\r\n",
      "句子对: 十月四日是什么星座\t四月四日是什么星座\t29920\t0\r\n",
      "句子对: 四的多少次方等七\t十的多少次方等七\t41253\t0\r\n",
      "句子对: 表格四次方怎么打出来\t表格十次方怎么打出来\t54653\t0\r\n",
      "句子对: 四mod两等多少\t十mod两等多少\t58590\t0\r\n",
      "句子对: 十字短句签名\t四字短句签名\t60658\t0\r\n",
      "句子对: 二十除几等十\t二十除几等四\t60675\t0\r\n",
      "句子对: 我国十大名画有哪些\t我国四大名画有哪些\t69615\t0\r\n",
      "句子对: 三点四除十等多少\t三点四除四等多少\t77636\t0\r\n"
     ]
    }
   ],
   "source": [
    "# 检测（1）对比两个句子是否拼音一致但书写不一致（2）有同音字不同的句子对（lazypinyin） \n",
    "from pypinyin import lazy_pinyin\n",
    "def compare_pinyin_and_text(text_list_1, text_list_2, text_label_list, print_progress = False):\n",
    "    special_pinyin_list = [[\"b\",\"p\"],[\"m\",\"n\"],[\"zh\",\"z\"],[\"ch\",\"c\"],[\"sh\",\"s\"],[\"r\",\"l\"],[\"r\",\"n\"],[\"n\",\"l\"],[\"en\",\"eng\"],[\"an\",\"ang\"],[\"in\",\"ing\"],[\"v\",\"i\"],[\"v\",\"u\"],[\"u\",\"i\"]]\n",
    "    special_words_list =[[\"四\",\"十\"],['汉','韩']]\n",
    "    special_sentence_list = [\"那里人\",\"哪里人\",\"哪人\",\"多少岁\"]\n",
    "    pinyin_0_list = []\n",
    "    label_0_list = []\n",
    "    pinyin_1_list = []\n",
    "    label_1_list = []\n",
    "    count_percent = 0\n",
    "    print(\"=\"*15 + \" In Progress \" + \"=\"*15)\n",
    "    for i in range (0, len(text_list_1)):\n",
    "        count_percent = count_percent + 1\n",
    "        \n",
    "        text_1 = text_list_1[i]\n",
    "        text_2 = text_list_2[i]\n",
    "\n",
    "        subset_1, subset_2 = check_subset(text_1, text_2)\n",
    "\n",
    "        contain_special_words = False\n",
    "        if special_words_list != []:\n",
    "            for special_word in special_words_list:\n",
    "                if (special_word[0] == subset_1 and special_word[1] == subset_2) or (special_word[0] == subset_2 and special_word[1] == subset_1):\n",
    "                    pinyin_0_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                    if text_label_list!= None:\n",
    "                        label_0_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "                    contain_special_words = True\n",
    "        contain_special_sentences = False\n",
    "        if special_sentence_list != []:\n",
    "            for special_sentence in special_sentence_list:\n",
    "                if special_sentence in text_1 and special_sentence in text_2:\n",
    "                    contain_special_sentences = True\n",
    "        # 整句话的拼音是否一致\n",
    "        if not contain_special_words and lazy_pinyin(text_1) == lazy_pinyin(text_2) and text_1 != text_2:\n",
    "            if contain_special_sentences:\n",
    "                pinyin_0_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                if text_label_list!= None:\n",
    "                    label_0_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "            else:\n",
    "                pinyin_1_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                if text_label_list!= None:\n",
    "                    label_1_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "        # 分析两句同样长度的话中不一样的部分拼音是否一致（因为整句话的识别 哪 nei 那 na等）\n",
    "        elif not contain_special_words and subset_1!=\"Not_The_Case\" and len(subset_1) > 0 and len(subset_1) == len(subset_2):\n",
    "            x = lazy_pinyin(subset_1)\n",
    "            y = lazy_pinyin(subset_2)\n",
    "            if x == y:\n",
    "                if contain_special_sentences:\n",
    "                    pinyin_0_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                    if text_label_list!= None:\n",
    "                        label_0_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "                else:\n",
    "                    pinyin_1_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                    if text_label_list!= None:\n",
    "                        label_1_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "            elif len(x) == len (y) and x != y:\n",
    "                for special_pinyin in special_pinyin_list:\n",
    "                    for index in range(0,len(x)):\n",
    "                        if special_pinyin[0] in x[index] and special_pinyin[0] not in y[index]:\n",
    "                            x[index] = x[index].replace(special_pinyin[0],special_pinyin[1])\n",
    "                        elif special_pinyin[0] in y[index] and special_pinyin[0] not in x[index]:\n",
    "                            y[index] = y[index].replace(special_pinyin[0],special_pinyin[1])\n",
    "                    if x == y :\n",
    "                        if contain_special_sentences:\n",
    "                            pinyin_0_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                            if text_label_list!= None:\n",
    "                                label_0_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "                        else:\n",
    "                            pinyin_1_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                            if text_label_list!= None:\n",
    "                                label_1_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "        \n",
    "        if print_progress and count_percent%10000 == 0:\n",
    "            print(\"%.2f of text pairs for different subsets identification have processed \" %((count_percent/len(text_list_1))*100))    \n",
    "    print(\"=\"*15 + \"     Done    \" + \"=\"*15)\n",
    "    return pinyin_1_list, label_1_list, pinyin_0_list, label_0_list\n",
    "\n",
    "# 使用 准备数据\n",
    "test_text_pair_1_list,test_text_pair_2_list = read_candidates_test(\"work/test_B_processed.txt\")\n",
    "view_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\")\n",
    "\n",
    "# 进行识别\n",
    "homophone_text_pairs_1_list, homophone_label_1_list, homophone_text_pairs_0_list, homophone_label_0_list = compare_pinyin_and_text(test_text_pair_1_list,test_text_pair_2_list, view_label_list, True)\n",
    "print(len(homophone_text_pairs_1_list))\n",
    "print(homophone_text_pairs_1_list[0])\n",
    "print(len(homophone_label_1_list))\n",
    "print(homophone_label_1_list[0])\n",
    "write_file(\"common_data/pinyin_1_text.txt\",homophone_text_pairs_1_list)\n",
    "write_file(\"common_data/pinyin_0_text.txt\",homophone_text_pairs_0_list)\n",
    "\n",
    "# 使用 方法2-7 统计测试集中，预测结果同音字的0/1概率\n",
    "test_zero_list, test_one_list = summarize_one_zero(homophone_text_pairs_1_list, homophone_label_1_list)\n",
    "print(\"\\none in homophone_text_pairs_1_list accounts for %.2f\" %(len(test_one_list)/len(homophone_text_pairs_1_list)*100))\n",
    "print(\"zero in homophone_text_pairs_1_list accounts for %.2f\" %(len(test_zero_list)/len(homophone_text_pairs_1_list)*100))\n",
    "print(len(test_one_list))\n",
    "print_examples(test_one_list, None, None, 50)\n",
    "print(len(test_zero_list))\n",
    "print_examples(test_zero_list, None, None, 50)\n",
    "\n",
    "test_zero_list, test_one_list = summarize_one_zero(homophone_text_pairs_0_list, homophone_label_0_list)\n",
    "if homophone_text_pairs_0_list != []:\n",
    "    print(\"\\none in homophone_text_pairs_0_list accounts for %.2f\" %(len(test_one_list)/len(homophone_text_pairs_0_list)*100))\n",
    "    print(\"zero in homophone_text_pairs_0_list accounts for %.2f\" %(len(test_zero_list)/len(homophone_text_pairs_0_list)*100))\n",
    "print(len(test_one_list))\n",
    "print_examples(test_one_list, None, None, 50)\n",
    "print(len(test_zero_list))\n",
    "print_examples(test_zero_list, None, None, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 检测有错别字的句子对（pycorrector）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T14:33:35.808280Z",
     "iopub.status.busy": "2024-10-15T14:33:35.807537Z",
     "iopub.status.idle": "2024-10-15T14:33:40.080602Z",
     "shell.execute_reply": "2024-10-15T14:33:40.078406Z",
     "shell.execute_reply.started": "2024-10-15T14:33:35.808239Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/, https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: pycorrector in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: loguru in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (0.7.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (1.21.6)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (0.42.1)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (4.30.2)\r\n",
      "Requirement already satisfied: pypinyin in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (0.53.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (1.16.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (1.1.5)\r\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (2.0.0)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pycorrector) (2.13.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (21.3)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (3.8.6)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (0.70.11.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (4.2.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (0.16.4)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (2023.1.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (5.1.2)\r\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (0.3.3)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (12.0.1)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (4.66.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets->pycorrector) (2.22.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->pycorrector) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->pycorrector) (2.8.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers->pycorrector) (3.0.12)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers->pycorrector) (2024.4.16)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers->pycorrector) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from transformers->pycorrector) (0.4.5)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (4.0.3)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (6.0.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (22.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (4.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (3.3.2)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (0.13.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets->pycorrector) (1.9.4)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets->pycorrector) (3.0.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets->pycorrector) (1.25.6)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets->pycorrector) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets->pycorrector) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets->pycorrector) (2019.9.11)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets->pycorrector) (3.8.1)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\r\n"
     ]
    }
   ],
   "source": [
    "pip install pycorrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T14:33:46.464617Z",
     "iopub.status.busy": "2024-10-15T14:33:46.463906Z",
     "iopub.status.idle": "2024-10-15T14:33:46.488072Z",
     "shell.execute_reply": "2024-10-15T14:33:46.486796Z",
     "shell.execute_reply.started": "2024-10-15T14:33:46.464581Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3555/2038048070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 检测有错别字的句子对（pycorrector）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# import pycorrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mernie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mernie_corrector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErnieCorrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErnieCorrector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pycorrector/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_corrector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfusionCorrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcontext_corrector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepContextCorrector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycorrector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUSER_DATA_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pycorrector/deepcontext/deepcontext_corrector.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloguru\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# 检测有错别字的句子对（pycorrector）\n",
    "# import pycorrector\n",
    "from pycorrector.ernie.ernie_corrector import ErnieCorrector\n",
    "\n",
    "m = ErnieCorrector()\n",
    "\n",
    "def identify_misspelling_pycorrector(text_list_1, text_list_2, text_label_list, print_progress = False):\n",
    "    text_pairs_list = []\n",
    "    subset_list = []\n",
    "    label_list = []\n",
    "    count_percent = 0\n",
    "    print(\"=\"*15 + \" In Progress \" + \"=\"*15)\n",
    "    for i in range (0, len(text_list_1)):\n",
    "        count_percent = count_percent + 1\n",
    "        \n",
    "        text_1 = text_list_1[i]\n",
    "        text_2 = text_list_2[i]\n",
    "\n",
    "        if text_1!=text_2 and len(text_1) == len(text_1):\n",
    "            corrected_sent_1, detail_1 = m.ernie_correct(text_1)\n",
    "            corrected_sent_2, detail_2 = m.ernie_correct(text_2)\n",
    "            if corrected_sent_1 == corrected_sent_2:\n",
    "                text_pairs_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                if text_label_list!= None:\n",
    "                    label_list.append(str(i) + \"\\t\" + str(text_label_list[i]))      \n",
    "        if print_progress and count_percent%5000 == 0:\n",
    "            print(\"%.2f of text pairs for pycorrect identification have processed \" %((count_percent/len(text_list_1))*100))    \n",
    "    print(\"=\"*15 + \"     Done    \" + \"=\"*15)\n",
    "    return text_pairs_list, label_list\n",
    "\n",
    "# 使用 准备数据\n",
    "test_text_pair_1_list,test_text_pair_2_list = read_candidates_test(\"work/test_B_processed.txt\")\n",
    "view_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\")\n",
    "\n",
    "# 进行识别\n",
    "pycorrect_text_pairs_list, pycorrect_label_list = identify_misspelling_pycorrector(test_text_pair_1_list,test_text_pair_2_list, view_label_list, True)\n",
    "print(len(pycorrect_text_pairs_list))\n",
    "print(pycorrect_text_pairs_list[0])\n",
    "print(len(pycorrect_label_list))\n",
    "print(pycorrect_label_list[0])\n",
    "write_file(\"common_data/pycorrector_1_list.txt\",pycorrect_text_pairs_list)\n",
    "print(len(read_candidates_line(\"common_data/pycorrector_1_list.txt\")))\n",
    "\n",
    "# 统计测试集中，预测结果同音字的0/1概率\n",
    "test_zero_list, test_one_list = summarize_one_zero(pycorrect_text_pairs_list, pycorrect_label_list)\n",
    "print(\"\\none in pycorrect_text_pairs_list accounts for %.2f\" %(len(test_one_list)/len(pycorrect_text_pairs_list)*100))\n",
    "print(\"zero in pycorrect_text_pairs_list accounts for %.2f\" %(len(test_zero_list)/len(pycorrect_text_pairs_list)*100))\n",
    "print(len(test_one_list))\n",
    "print_examples(test_one_list, None, None, 5)\n",
    "print(len(test_zero_list))\n",
    "print_examples(test_zero_list, None, None, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 检测有错别字的句子对(去除部首后对比）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:39:33.926080Z",
     "iopub.status.busy": "2024-10-14T02:39:33.925090Z",
     "iopub.status.idle": "2024-10-14T02:39:33.935777Z",
     "shell.execute_reply": "2024-10-14T02:39:33.934185Z",
     "shell.execute_reply.started": "2024-10-14T02:39:33.926007Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 判断一个的笔画是否包含在另一个字的笔画中\n",
    "def check_word_contain(a, b, bihua_1, bushou_1, bihua_2, bushou_2):\n",
    "    is_contain = True\n",
    "    long_bihua = []\n",
    "    short_bihua = []\n",
    "    different_bihua = []\n",
    "    long_bushou = []\n",
    "    # 对比两个字的笔画，将笔画多的放入long_bihua其部首放入long_bushou，少的放入short_bihua,\n",
    "    if len(bihua_1) > len(bihua_2):\n",
    "        long_bihua = bihua_1\n",
    "        short_bihua = bihua_2\n",
    "        long_bushou = bushou_1\n",
    "    else:\n",
    "        long_bihua = bihua_2\n",
    "        short_bihua = bihua_1\n",
    "        long_bushou = bushou_2\n",
    "    \n",
    "    # 情况1 如果这两个字的从第一笔开始到len（short_bihua）存在不同，则为 不包含情况1\n",
    "    for i in range(0, len(short_bihua)):\n",
    "        if short_bihua[i] != long_bihua[i]:\n",
    "            is_contain = False\n",
    "    \n",
    "    # 如果这两个字的从第一笔开始到len（short_bihua）都相同，则为 包含 \n",
    "    if is_contain == True:\n",
    "        # 判断剩下的笔画是否为多笔画字的部首\n",
    "        different_bihua = long_bihua[len(short_bihua):len(long_bihua)]\n",
    "        # 若不同部分仅为部首，返回True。否则False\n",
    "        if different_bihua == long_bushou:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        is_contain = True\n",
    "        # 情况2 如果这两个字的从len（short_bihua） 到 最后 存在不同，则为 不包含情况2\n",
    "        for i in range(0, len(short_bihua)):\n",
    "            if short_bihua[i] != long_bihua[i + len(long_bihua) - len(short_bihua)]:\n",
    "                is_contain = False\n",
    "        # 如果这两个字的从len（short_bihua） 到 最后 都相同，则为 包含 \n",
    "        if is_contain == True:\n",
    "            # 判断剩下的笔画是否为多笔画字的部首\n",
    "            different_bihua = long_bihua[0:len(long_bihua) - len(short_bihua)]\n",
    "            # 若不同部分仅为部首，返回True。否则False\n",
    "            if different_bihua == long_bushou:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:39:47.321064Z",
     "iopub.status.busy": "2024-10-14T02:39:47.320405Z",
     "iopub.status.idle": "2024-10-14T02:39:47.329387Z",
     "shell.execute_reply": "2024-10-14T02:39:47.328373Z",
     "shell.execute_reply.started": "2024-10-14T02:39:47.321020Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 去掉两个字符串中相同的连续部分\n",
    "def remove_bushou(a, b, text_1, bushou_1, text_2, bushou_2):\n",
    "\n",
    "    if text_1!= bushou_1 and text_2 != bushou_2:\n",
    "        #获取除了部首之外的笔画\n",
    "        rest_1 = []\n",
    "        rest_2 = []\n",
    "\n",
    "        # print(\"text_1: \" + str(text_1))\n",
    "        # print(\"bushou_1:\"  + str(bushou_1))\n",
    "        # print(\"text_2: \" + str(text_2))\n",
    "        # print(\"bushou_2: \" + str(bushou_2))\n",
    "\n",
    "        subset_1_1, subset_1_2 = check_subset(text_1, bushou_1)\n",
    "        subset_2_1, subset_2_2 = check_subset(text_2, bushou_2)\n",
    "        \n",
    "        # print(subset_1_1,subset_1_2 )\n",
    "        # print(subset_2_1,subset_2_2 )\n",
    "        # 第一个字去掉部首后剩余的笔画\n",
    "        if subset_1_1 == \"Not_The_Case\" or subset_1_2 == \"Not_The_Case\":\n",
    "            x = 1\n",
    "        elif subset_1_2 ==[] and subset_1_1!=[]: \n",
    "            rest_1 = subset_1_1 \n",
    "        elif subset_1_2 !=[] and subset_1_1==[]:\n",
    "            rest_1 = subset_1_2\n",
    "\n",
    "        # 第二个字去掉部首后剩余的笔画\n",
    "        if subset_2_1 == \"Not_The_Case\" or subset_2_2 == \"Not_The_Case\":\n",
    "            x = 1\n",
    "        elif subset_2_2 ==[] and subset_2_1!=[]: \n",
    "            rest_2 = subset_2_1 \n",
    "        elif subset_2_2 !=[] and subset_2_1==[]:\n",
    "            rest_2 = subset_2_2\n",
    "\n",
    "        \n",
    "        # print(rest_1,rest_2)  \n",
    "        #若剩余比划相同 且 不为空\n",
    "        if rest_1 == rest_2 and rest_1 != []:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # 若分解出的部首就是该字本身\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:39:51.092100Z",
     "iopub.status.busy": "2024-10-14T02:39:51.091476Z",
     "iopub.status.idle": "2024-10-14T02:39:51.103836Z",
     "shell.execute_reply": "2024-10-14T02:39:51.102078Z",
     "shell.execute_reply.started": "2024-10-14T02:39:51.092062Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#通过cixing来分解汉字部首和笔画\n",
    "from cixing.cixing_5 import *\n",
    "def compare_bushou_bihua(text_1,text_2):\n",
    "    handler = ChineseCixing()\n",
    "    #获得全字的笔画\n",
    "    strokes_original_1 = handler.get_strokes(text_1)\n",
    "    strokes_original_2 = handler.get_strokes(text_2)\n",
    "    #获得全字的部首\n",
    "    radicals_original_1 = handler.get_radical(text_1)\n",
    "    radicals_original_2 = handler.get_radical(text_2)\n",
    "    #获得部首的笔画\n",
    "    radicals_bushou_1 = handler.get_strokes(radicals_original_1)\n",
    "    radicals_bushou_2 = handler.get_strokes(radicals_original_2)\n",
    "    #如果其中一个字是另一个字的一部分，如 不/怀 返回True\n",
    "    if check_word_contain(text_1, text_2, strokes_original_1[0],radicals_bushou_1[0],strokes_original_2[0],radicals_bushou_2[0]) :\n",
    "        # print(\"其中一个字是另一个字去掉部首后的部分\")\n",
    "        return True\n",
    "    else:\n",
    "        # 部首不同 如 怀/还\n",
    "        if radicals_original_1[0] != radicals_original_2[0]:\n",
    "            # print(\"部首不同\")\n",
    "            comparison = remove_bushou(text_1, text_2, strokes_original_1[0],radicals_bushou_1[0],strokes_original_2[0],radicals_bushou_2[0])\n",
    "            return comparison\n",
    "        else:\n",
    "            # print(\"部首相同\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:39:56.287133Z",
     "iopub.status.busy": "2024-10-14T02:39:56.286508Z",
     "iopub.status.idle": "2024-10-14T02:39:56.296655Z",
     "shell.execute_reply": "2024-10-14T02:39:56.295885Z",
     "shell.execute_reply.started": "2024-10-14T02:39:56.287096Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 检测仅一字不同，且除部首外有相似结构\n",
    "# 是否是中文\n",
    "def is_Chinese(ch): \n",
    "    if '\\u4e00' <= ch <= '\\u9fff': \n",
    "        return True \n",
    "    return False\n",
    "\n",
    "# 方法3-3 检测仅一字不同，且除部首外有相似结构\n",
    "def identify_misspelling_structure(text_list_1, text_list_2, text_label_list, print_progress = False):\n",
    "    text_pairs_list = []\n",
    "    label_list = []\n",
    "    print(\"=\"*15 + \" In Progress \" + \"=\"*15)\n",
    "    for i in range (0, len(text_list_1)):\n",
    "\n",
    "        text_1 = text_list_1[i]\n",
    "        text_2 = text_list_2[i]\n",
    "\n",
    "        subset_1, subset_2 = check_subset(text_1, text_2)\n",
    "        if subset_1!=\"Not_The_Case\" and subset_1!=\"\" and subset_2!=\"\":\n",
    "            # 如果有一个字不一样，尝试发现是否是结构上的错别字\n",
    "            if len(subset_1) == len(subset_2) and len(subset_2) == 1 :\n",
    "                if is_Chinese(subset_1) and is_Chinese(subset_2) and not(subset_1.isnumeric() or subset_2.isnumeric()):\n",
    "                    has_similar_part = compare_bushou_bihua(subset_1,subset_2)\n",
    "                    if has_similar_part:\n",
    "                        text_pairs_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                        if text_label_list!= None:\n",
    "                            label_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "        if print_progress and i!=0 and i%2000 == 0:\n",
    "            print(\"%.2f %of text pairs for 1 word different have processed \" %((i/len(text_list_1))*100))  \n",
    "    print(\"=\"*15 + \"     Done    \" + \"=\"*15)\n",
    "    return text_pairs_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:40:00.372711Z",
     "iopub.status.busy": "2024-10-14T02:40:00.372082Z",
     "iopub.status.idle": "2024-10-14T03:15:59.546899Z",
     "shell.execute_reply": "2024-10-14T03:15:59.545716Z",
     "shell.execute_reply.started": "2024-10-14T02:40:00.372653Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The number of lines in work/test_B_processed.txt is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "=============== In Progress ===============\r\n",
      "2.00 of text pairs for 1 word different have processed \r\n",
      "4.00 of text pairs for 1 word different have processed \r\n",
      "6.00 of text pairs for 1 word different have processed \r\n",
      "8.00 of text pairs for 1 word different have processed \r\n",
      "10.00 of text pairs for 1 word different have processed \r\n",
      "12.00 of text pairs for 1 word different have processed \r\n",
      "14.00 of text pairs for 1 word different have processed \r\n",
      "16.00 of text pairs for 1 word different have processed \r\n",
      "18.00 of text pairs for 1 word different have processed \r\n",
      "20.00 of text pairs for 1 word different have processed \r\n",
      "22.00 of text pairs for 1 word different have processed \r\n",
      "24.00 of text pairs for 1 word different have processed \r\n",
      "26.00 of text pairs for 1 word different have processed \r\n",
      "28.00 of text pairs for 1 word different have processed \r\n",
      "30.00 of text pairs for 1 word different have processed \r\n",
      "32.00 of text pairs for 1 word different have processed \r\n",
      "34.00 of text pairs for 1 word different have processed \r\n",
      "36.00 of text pairs for 1 word different have processed \r\n",
      "38.00 of text pairs for 1 word different have processed \r\n",
      "40.00 of text pairs for 1 word different have processed \r\n",
      "42.00 of text pairs for 1 word different have processed \r\n",
      "44.00 of text pairs for 1 word different have processed \r\n",
      "46.00 of text pairs for 1 word different have processed \r\n",
      "48.00 of text pairs for 1 word different have processed \r\n",
      "50.00 of text pairs for 1 word different have processed \r\n",
      "52.00 of text pairs for 1 word different have processed \r\n",
      "54.00 of text pairs for 1 word different have processed \r\n",
      "56.00 of text pairs for 1 word different have processed \r\n",
      "58.00 of text pairs for 1 word different have processed \r\n",
      "60.00 of text pairs for 1 word different have processed \r\n",
      "62.00 of text pairs for 1 word different have processed \r\n",
      "64.00 of text pairs for 1 word different have processed \r\n",
      "66.00 of text pairs for 1 word different have processed \r\n",
      "68.00 of text pairs for 1 word different have processed \r\n",
      "70.00 of text pairs for 1 word different have processed \r\n",
      "72.00 of text pairs for 1 word different have processed \r\n",
      "74.00 of text pairs for 1 word different have processed \r\n",
      "76.00 of text pairs for 1 word different have processed \r\n",
      "78.00 of text pairs for 1 word different have processed \r\n",
      "80.00 of text pairs for 1 word different have processed \r\n",
      "82.00 of text pairs for 1 word different have processed \r\n",
      "84.00 of text pairs for 1 word different have processed \r\n",
      "86.00 of text pairs for 1 word different have processed \r\n",
      "88.00 of text pairs for 1 word different have processed \r\n",
      "90.00 of text pairs for 1 word different have processed \r\n",
      "92.00 of text pairs for 1 word different have processed \r\n",
      "94.00 of text pairs for 1 word different have processed \r\n",
      "96.00 of text pairs for 1 word different have processed \r\n",
      "98.00 of text pairs for 1 word different have processed \r\n",
      "===============     Done    ===============\r\n",
      "1019\r\n",
      "水浒传作者\t水浒转作者\r\n",
      "1019\r\n",
      "166\t1\r\n",
      "\r\n",
      "one in structure_text_pairs_list accounts for 99.31\r\n",
      "zero in structure_text_pairs_list accounts for 0.69\r\n",
      "1012\r\n",
      "句子对: 水浒传作者\t水浒转作者\t166\t1\r\n",
      "句子对: 券养的意思\t劵养的意思\t185\t1\r\n",
      "句子对: 额头长痘痘是什么原因\t额头张痘痘是什么原因\t192\t1\r\n",
      "句子对: 猕猴桃能和酸奶一起吃吗\t弥猴桃能和酸奶一起吃吗\t308\t1\r\n",
      "句子对: 甲醛有气味吗\t甲荃有气味吗\t338\t1\r\n",
      "句子对: 腰骶部酸痛怎么回事\t腰底部酸痛怎么回事\t350\t1\r\n",
      "7\r\n",
      "句子对: 药品怎么读\t药品怎么卖\t13355\t0\r\n",
      "句子对: 偏头痛是怎么引起的\t遍头痛是怎么引起的\t24565\t0\r\n",
      "句子对: 落枕脖子疼怎么办\t洛枕脖子疼怎么办\t39739\t0\r\n",
      "句子对: 脸上皮肤痒是怎么回事\t脸上皮肤庠是怎么回事\t43468\t0\r\n",
      "句子对: 内眼角痒是怎么回事\t内眼角庠是怎么回事\t65712\t0\r\n",
      "句子对: 撒的多音字组字\t散的多音字组字\t68807\t0\r\n"
     ]
    }
   ],
   "source": [
    "# 使用 准备数据\n",
    "test_text_pair_1_list,test_text_pair_2_list = read_candidates_test(\"work/test_B_processed.txt\")\n",
    "view_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\")\n",
    "\n",
    "# 进行识别\n",
    "structure_text_pairs_list, structure_label_list = identify_misspelling_structure(test_text_pair_1_list,test_text_pair_2_list, view_label_list, True)\n",
    "print(len(structure_text_pairs_list))\n",
    "print(structure_text_pairs_list[0])\n",
    "print(len(structure_label_list))\n",
    "print(structure_label_list[0])\n",
    "\n",
    "write_file(\"common_data/structure_1_list.txt\",structure_text_pairs_list)\n",
    "\n",
    "# 统计测试集中，预测结果同音字的0/1概率\n",
    "test_zero_list, test_one_list = summarize_one_zero(structure_text_pairs_list, structure_label_list)\n",
    "print(\"\\none in structure_text_pairs_list accounts for %.2f\" %(len(test_one_list)/len(structure_text_pairs_list)*100))\n",
    "print(\"zero in structure_text_pairs_list accounts for %.2f\" %(len(test_zero_list)/len(structure_text_pairs_list)*100))\n",
    "print(len(test_one_list))\n",
    "print_examples(test_one_list, None, None, 5)\n",
    "print(len(test_zero_list))\n",
    "print_examples(test_zero_list, None, None, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 同音人名与同音地名不作为mispelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:18:02.485341Z",
     "iopub.status.busy": "2024-10-14T03:18:02.484467Z",
     "iopub.status.idle": "2024-10-14T03:18:30.335131Z",
     "shell.execute_reply": "2024-10-14T03:18:30.334225Z",
     "shell.execute_reply.started": "2024-10-14T03:18:02.485302Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The number of lines in work/test_B_processed.txt is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "=============== In Progress ===============\r\n",
      "0.00 of text pairs for name misspelling identification have processed \r\n",
      "10.00 of text pairs for name misspelling identification have processed \r\n",
      "20.00 of text pairs for name misspelling identification have processed \r\n",
      "30.00 of text pairs for name misspelling identification have processed \r\n",
      "40.00 of text pairs for name misspelling identification have processed \r\n",
      "50.00 of text pairs for name misspelling identification have processed \r\n",
      "60.00 of text pairs for name misspelling identification have processed \r\n",
      "70.00 of text pairs for name misspelling identification have processed \r\n",
      "80.00 of text pairs for name misspelling identification have processed \r\n",
      "90.00 of text pairs for name misspelling identification have processed \r\n",
      "===============     Done    ===============\r\n",
      "带有相关信息的名人名字拼写错误的有: 0 对\r\n",
      "\r\n",
      "==============0==============\r\n",
      "==============390==============\r\n",
      "\r\n",
      "one in name_0_text_pairs_list accounts for 6.67\r\n",
      "zero in name_0_text_pairs_list accounts for 93.33\r\n",
      "句子对: 章子怡是那里人\t张子怡是那里人\t2284\t1\r\n",
      "句子对: 王子怡是谁\t汪子怡是谁\t4449\t1\r\n",
      "句子对: 乌兰图雅的老公是谁\t乌蓝图雅的老公是谁\t6510\t1\r\n",
      "句子对: 伊可尔治疗扁平疣\t伊可儿治疗扁平疣\t6982\t1\r\n",
      "句子对: 程咬金是什么生肖\t程咬何是什么生肖\t14676\t1\r\n",
      "句子对: 刘亚的拼音\t刘娅的拼音\t19050\t1\r\n",
      "句子对: 大空翼是哪个球队的\t大平翼是哪个球队的\t19052\t1\r\n",
      "句子对: 刘晓燕哪里人\t刘小燕哪里人\t21532\t1\r\n",
      "句子对: 什么玛丽亚\t什么玛利亚\t27038\t1\r\n",
      "句子对: 武则天原籍是哪里\t常则天原籍是哪里\t30955\t1\r\n",
      "句子对: 刘恺威多少岁数\t刘凯威多少岁数\t32870\t1\r\n",
      "句子对: 金三顺作品\t金叁顺作品\t33513\t1\r\n",
      "句子对: 朱元漳为什么要大杀功臣\t朱元璋为什么要大杀功臣\t34693\t1\r\n",
      "句子对: 贾玲多少岁了\t嘉玲多少岁了\t37070\t1\r\n",
      "句子对: 张子枫多大\t章子枫多大\t39366\t1\r\n",
      "句子对: 王健林哪里人\t王建林哪里人\t49319\t1\r\n",
      "句子对: 余秋雨的作品\t于秋雨的作品\t49334\t1\r\n",
      "句子对: 杨幂和刘恺威真的复合了吗\t杨幂和刘凯威真的复合了吗\t56730\t1\r\n",
      "句子对: 张子枫多大\t张子风多大\t59650\t1\r\n",
      "句子对: 唐嫣哪里人\t唐岩哪里人\t65984\t1\r\n",
      "句子对: 朱德是那里人\t朱徳是那里人\t66374\t1\r\n",
      "句子对: 萨达姆是哪国的\t撒达姆是哪国的\t75613\t1\r\n",
      "句子对: 陈薇院士研究的疫苗属于哪类疫苗\t陈微院士研究的疫苗属于哪类疫苗\t80043\t1\r\n",
      "句子对: 最强nba钻石艾弗森怎么加点\t最强nba钻石艾佛森怎么加点\t82244\t1\r\n",
      "句子对: 刘昊然演的综艺\t刘浩然演的综艺\t86822\t1\r\n",
      "句子对: 叶绍翁是哪个朝代的\t叶少翁是哪个朝代的\t99232\t1\r\n",
      "句子对: 唐国强岁数\t张国强岁数\t187\t0\r\n",
      "句子对: 吴生的拼音\t厉生的拼音\t290\t0\r\n",
      "句子对: 司马懿是哪里人\t司马徽是哪里人\t559\t0\r\n",
      "句子对: 贾乃亮演唱追光者\t鲍乃亮演唱追光者\t774\t0\r\n",
      "句子对: 评价诸葛亮\t评价诸葛瞻\t2005\t0\r\n",
      "句子对: 宋庆龄哪里人\t殳庆龄哪里人\t2242\t0\r\n",
      "句子对: 黄日华的女儿\t黄旭华的女儿\t2352\t0\r\n",
      "句子对: 李涵这个名字怎么样\t杨涵这个名字怎么样\t2422\t0\r\n",
      "句子对: 给周红打电话\t给周鑫打电话\t2649\t0\r\n",
      "句子对: 李华是什么朝代\t李白是什么朝代\t3358\t0\r\n",
      "句子对: 钟汉良的老婆是谁呀\t曲汉良的老婆是谁呀\t3598\t0\r\n",
      "句子对: 于震演过的电视剧\t于波演过的电视剧\t3600\t0\r\n",
      "句子对: 黄明昊今年多少岁\t杨明昊今年多少岁\t3678\t0\r\n",
      "句子对: 张韶涵的音域有多宽\t张米涵的音域有多宽\t3941\t0\r\n",
      "句子对: 李菁菁老公\t卫菁菁老公\t4052\t0\r\n",
      "句子对: 打电话给万总\t打电话给陆总\t4229\t0\r\n",
      "句子对: 范冰冰整容了吗\t李冰冰整容了吗\t4371\t0\r\n",
      "句子对: 吕丽萍的老公是谁\t赵丽萍的老公是谁\t4605\t0\r\n",
      "句子对: 孔融是个什么样的人\t雷融是个什么样的人\t4800\t0\r\n",
      "句子对: 杨万里是哪个朝代的\t杨咸里是哪个朝代的\t5017\t0\r\n",
      "句子对: 于震演过的电视剧\t陈震演过的电视剧\t5243\t0\r\n",
      "句子对: 鲁班发明了什么\t鲁康发明了什么\t5822\t0\r\n",
      "句子对: 李菁菁老公\t秋菁菁老公\t6016\t0\r\n",
      "句子对: 司马迁名言\t司马衷名言\t6095\t0\r\n",
      "句子对: 杨千里演员\t杨千嬅演员\t6172\t0\r\n",
      "句子对: 王羲之是哪个朝代的人\t王献之是哪个朝代的人\t6304\t0\r\n",
      "句子对: 爱因斯坦发明了什么\t禹因斯坦发明了什么\t6621\t0\r\n",
      "句子对: 周文王活了多少岁\t周宣王活了多少岁\t6707\t0\r\n",
      "句子对: 李小林丈夫是谁\t布小林丈夫是谁\t6838\t0\r\n",
      "句子对: 诸葛亮的儿子是谁\t诸葛瞻的儿子是谁\t6894\t0\r\n",
      "句子对: 朱开山是哪个地方的人\t朱开福是哪个地方的人\t7324\t0\r\n",
      "句子对: 孟浩然是哪里人\t木浩然是哪里人\t7446\t0\r\n",
      "句子对: 靳东演的电视剧\t晓东演的电视剧\t7995\t0\r\n",
      "句子对: 打电话给琴琴\t打电话给陈琴\t8103\t0\r\n",
      "句子对: 刘德华最新电影有哪一些\t桓德华最新电影有哪一些\t8133\t0\r\n",
      "句子对: 卓别林为什么没有被杀\t卓师林为什么没有被杀\t9151\t0\r\n",
      "句子对: 左小欣扮演者\t左小青扮演者\t9198\t0\r\n",
      "句子对: 李白的诗句\t李晨的诗句\t9824\t0\r\n",
      "句子对: 杨靖宇是一个怎么样的人\t袁靖宇是一个怎么样的人\t10134\t0\r\n",
      "句子对: 赵本山有上春晚吗\t赵本桓有上春晚吗\t10550\t0\r\n",
      "句子对: 杨靖宇是一个怎么样的人\t孔靖宇是一个怎么样的人\t10633\t0\r\n",
      "句子对: 演员傅亨年龄\t演员傅淼年龄\t10988\t0\r\n",
      "句子对: 找梁静茹情歌的歌词要完整版的\t找梁静覃情歌的歌词要完整版的\t11641\t0\r\n",
      "句子对: 苏轼哪个朝代\t姚轼哪个朝代\t12137\t0\r\n",
      "句子对: 崔子是什么意思\t柴子是什么意思\t12606\t0\r\n",
      "句子对: 张无忌和张三丰谁厉害\t张无忌和张三史谁厉害\t13111\t0\r\n",
      "句子对: 雷锋祭日是哪天\t桓锋祭日是哪天\t14446\t0\r\n",
      "句子对: 程颖是什么朝代\t戴颖是什么朝代\t14646\t0\r\n",
      "句子对: 杨乐乐哪里人\t胡乐乐哪里人\t15058\t0\r\n",
      "句子对: 杜甫是哪个朝代的诗人\t杜牧是哪个朝代的诗人\t15100\t0\r\n",
      "句子对: 孔子老师是谁\t木子老师是谁\t15950\t0\r\n"
     ]
    }
   ],
   "source": [
    "# 尝试发现仅有人名是否仅有一个字的区别，若有则返回True\n",
    "def discover_naming_mistake(name_1, name_2):\n",
    "    different_words = []\n",
    "    if len(name_1) == len(name_2) and len(name_2)>=2:\n",
    "        for i in range(0, len(name_1)):\n",
    "            if name_1[i] != name_2[i]:\n",
    "                different_words.append(name_1[i])\n",
    "                different_words.append(name_2[i])\n",
    "    if len(different_words) == 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 方法 3-4 寻找包含人名但仅差别一个字的句子对\n",
    "from pypinyin import lazy_pinyin\n",
    "def identify_name_misspelling(data_list_1, data_list_2, data_label_list = None, print_progress = False):\n",
    "    celebrities_list = []\n",
    "    name_1_text_pairs_list = []\n",
    "    name_1_label_list = []\n",
    "    name_0_text_pairs_list = []\n",
    "    name_0_label_list = []\n",
    "    print(\"=\"*15 + \" In Progress \" + \"=\"*15)\n",
    "    count = 0\n",
    "    for i in range (0, len(data_list_1)):\n",
    "        text_1 = data_list_1[i]\n",
    "        text_2 = data_list_2[i]\n",
    "\n",
    "        subset_1, subset_2 = check_subset(text_1,text_2)\n",
    "\n",
    "        is_same_name_entity = False\n",
    "\n",
    "        if subset_1!=\"Not_The_Case\" and len(subset_1) == 1 and len(subset_1) == len(subset_2):\n",
    "            words_1, natures_1 = get_words_and_words_nature(text_1)\n",
    "            words_2, natures_2 = get_words_and_words_nature(text_2)\n",
    "            # 有特殊姓名的处理方法\n",
    "            if celebrities_list != []:\n",
    "                if \"PER\" in natures_1 and \"PER\" in natures_2 and len(natures_1) == len(natures_2):\n",
    "                    # print(subset_1,subset_2)\n",
    "                    for x in range(0, len(natures_1)):\n",
    "                        if (natures_1[x] == \"PER\" and natures_2[x] == \"PER\") and (words_1[x] != words_2[x]):\n",
    "                            # print(words_1[x] , words_2[x])\n",
    "                            for celebrity in celebrities_list:\n",
    "                                condition_1 = bool((celebrity == words_1[x]) and discover_naming_mistake(celebrity, words_2[x]))\n",
    "                                condition_2 = bool((celebrity == words_2[x]) and discover_naming_mistake(celebrity, words_1[x]))\n",
    "                                condition_3 = bool((words_1[x] in celebrity) and (celebrity in words_2[x]))\n",
    "                                condition_4 = bool((words_2[x] in celebrity) and (celebrity in words_1[x]))\n",
    "                                if  condition_1 or condition_2 or condition_3 or condition_4:\n",
    "                                    count = count + 1\n",
    "                                    name_1_text_pairs_list.append(data_list_1[i] + \"\\t\" + data_list_2[i] )\n",
    "                                    if data_label_list!= None:\n",
    "                                        name_1_label_list.append(str(i) + \"\\t\" + data_label_list[i])\n",
    "                                        is_same_name_entity = True\n",
    "                                    break\n",
    "                    for x in range(0, len(natures_1)):\n",
    "                        if (natures_1[x] == \"PER\" and natures_2[x] == \"PER\") and (words_1[x] != words_2[x]) and not is_same_name_entity:\n",
    "                            name_0_text_pairs_list.append(data_list_1[i] + \"\\t\" + data_list_2[i] )\n",
    "                            if data_label_list!= None:\n",
    "                                name_0_label_list.append(str(i) + \"\\t\" + data_label_list[i])\n",
    "            else:\n",
    "                if \"PER\" in natures_1 and \"PER\" in natures_2 and len(natures_1) == len(natures_2):\n",
    "                    # print(subset_1,subset_2)\n",
    "                    for x in range(0, len(natures_1)):\n",
    "                        if (natures_1[x] == \"PER\" and natures_2[x] == \"PER\") and (words_1[x] != words_2[x]):\n",
    "                            name_0_text_pairs_list.append(data_list_1[i] + \"\\t\" + data_list_2[i] )\n",
    "                            if data_label_list!= None:\n",
    "                                name_0_label_list.append(str(i) + \"\\t\" + data_label_list[i]) \n",
    "        if print_progress and i%10000 == 0:\n",
    "            print(\"%.2f of text pairs for name misspelling identification have processed \" %((i/len(data_list_1))*100))    \n",
    "    print(\"=\"*15 + \"     Done    \" + \"=\"*15)\n",
    "    print(\"带有相关信息的名人名字拼写错误的有: \" + str(count) + \" 对\\n\")\n",
    "    return name_1_text_pairs_list, name_1_label_list, name_0_text_pairs_list, name_0_label_list\n",
    "\n",
    "# 使用 准备数据\n",
    "test_text_pair_1_list,test_text_pair_2_list = read_candidates_test(\"work/test_B_processed.txt\")\n",
    "view_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\") #仅仅用来查看，不影响结果\n",
    "\n",
    "# 进行识别\n",
    "name_1_text_pairs_list, name_1_label_list, name_0_text_pairs_list, name_0_label_list = identify_name_misspelling(test_text_pair_1_list,test_text_pair_2_list, view_label_list, True)\n",
    "\n",
    "write_file(\"common_data/name_1_list.txt\",name_1_text_pairs_list)\n",
    "write_file(\"common_data/name_0_list.txt\",name_0_text_pairs_list)\n",
    "\n",
    "# 统计测试集中，预测结果同音字的0/1概率\n",
    "\n",
    "print(\"==============\" + str(len(name_1_text_pairs_list)) + \"==============\")\n",
    "test_zero_list, test_one_list = summarize_one_zero(name_1_text_pairs_list, name_1_label_list)\n",
    "if len(name_1_text_pairs_list)!=0:\n",
    "    print(\"\\none in name_1_text_pairs_list accounts for %.2f\" %(len(test_one_list)/len(name_1_text_pairs_list)*100))\n",
    "    print(\"zero in name_1_text_pairs_list accounts for %.2f\" %(len(test_zero_list)/len(name_1_text_pairs_list)*100))\n",
    "print_examples(test_one_list, None, None, 50)\n",
    "print_examples(test_zero_list, None, None, 50)\n",
    "\n",
    "print(\"==============\" + str(len(name_0_text_pairs_list)) + \"==============\")\n",
    "test_zero_list, test_one_list = summarize_one_zero(name_0_text_pairs_list, name_0_label_list)\n",
    "if len(name_0_text_pairs_list)!=0:\n",
    "    print(\"\\none in name_0_text_pairs_list accounts for %.2f\" %(len(test_one_list)/len(name_0_text_pairs_list)*100))\n",
    "    print(\"zero in name_0_text_pairs_list accounts for %.2f\" %(len(test_zero_list)/len(name_0_text_pairs_list)*100))\n",
    "print_examples(test_one_list, None, None, 50)\n",
    "print_examples(test_zero_list, None, None, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:19:05.272294Z",
     "iopub.status.busy": "2024-10-14T03:19:05.271360Z",
     "iopub.status.idle": "2024-10-14T03:24:19.700623Z",
     "shell.execute_reply": "2024-10-14T03:24:19.699734Z",
     "shell.execute_reply.started": "2024-10-14T03:19:05.272254Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\r\n",
      "391\r\n",
      "3227\r\n",
      "3088\r\n",
      "阳东\r\n",
      "0.00 of locations for homophone have processed \r\n",
      "8.10 of locations for homophone have processed \r\n",
      "16.19 of locations for homophone have processed \r\n",
      "24.29 of locations for homophone have processed \r\n",
      "32.38 of locations for homophone have processed \r\n",
      "40.48 of locations for homophone have processed \r\n",
      "48.58 of locations for homophone have processed \r\n",
      "56.67 of locations for homophone have processed \r\n",
      "64.77 of locations for homophone have processed \r\n",
      "72.86 of locations for homophone have processed \r\n",
      "80.96 of locations for homophone have processed \r\n",
      "89.05 of locations for homophone have processed \r\n",
      "97.15 of locations for homophone have processed \r\n",
      "325\r\n",
      "['大观\\t大关', '广灵\\t广陵', '细河\\t西和', '西夏\\t西峡', '伊吾\\t义乌', '萍乡\\t凭祥', '萍乡\\t平乡', '拜城\\t白城', '肃州\\t苏州', '肃州\\t宿州', '黔江\\t潜江', '淇县\\t杞县', '潞西\\t泸西', '潞西\\t芦溪', '潞西\\t泸溪', '睢宁\\t遂宁', '睢宁\\t绥宁', '富源\\t抚远', '鸡西\\t绩溪', '延津\\t盐津', '香洲\\t象州', '浔阳\\t旬阳', '龙华\\t隆化', '睢阳\\t绥阳', '浦城\\t蒲城', '平南\\t屏南', '玄武\\t宣武', '陆丰\\t禄丰', '龙南\\t陇南', '洛江\\t罗江', '简阳\\t建阳', '迎江\\t盈江', '袁州\\t原州', '明山\\t名山', '泊头\\t坡头', '襄城\\t芗城', '襄城\\t乡城', '襄城\\t相城', '襄城\\t项城', '濉溪\\t遂溪', '屏山\\t平山', '枝江\\t芷江', '丽水\\t溧水', '荣县\\t容县', '三元\\t三原', '同江\\t通江', '昌平\\t常平', '郧西\\t云溪', '吴江\\t武江', '安源\\t安远', '文县\\t温县', '鄢陵\\t炎陵', '新沂\\t信义', '新沂\\t信宜', '临翔\\t临湘', '固原\\t沽源', '晴隆\\t青龙', '安西\\t安溪', '虞城\\t雨城', '虞城\\t禹城', '阜城\\t涪城', '泰和\\t太和', '利川\\t黎川', '阜平\\t佛坪', '阜平\\t富平', '清浦\\t青浦', '蒲江\\t浦江', '山西\\t陕西', '桐城\\t通城', '潞城\\t鹿城', '武城\\t婺城', '镇沅\\t镇原', '镇沅\\t镇远', '高平\\t高坪', '礼县\\t理县', '礼县\\t澧县', '溪湖\\t西湖', '振安\\t镇安', '游仙\\t攸县', '遂宁\\t绥宁', '泸西\\t芦溪', '泸西\\t泸溪', '六合\\t柳河', '富宁\\t阜宁', '富宁\\t抚宁', '海盐\\t海晏', '恒山\\t衡山', '恒山\\t横山', '朝阳\\t昭阳', '铜仁\\t同仁', '元江\\t沅江', '雁山\\t砚山', '雁山\\t盐山', '雁山\\t铅山', '蕉城\\t交城', '龙港\\t龙岗', '静宁\\t景宁', '南长\\t南漳', '大荔\\t大理', '银州\\t鄞州',\r\n"
     ]
    }
   ],
   "source": [
    "# 同音地名处理（无需重复运行）\n",
    "# 去除 地名后的\"省\",\"市\",\"县\",\"区\",\"镇\",\"乡\",\"村\"\n",
    "def remove_p_c_a(location):\n",
    "    p_c_t = [\n",
    "        \"拉祜族佤族布朗族傣族自治县\",\"保安族东乡族撒拉族自治县\",\"彝族哈尼族拉祜族自治县\",\"傣族拉祜族佤族自治县\",\"哈尼族彝族傣族自治县\",\n",
    "        \"哈尼族彝族傣族自治县\",\"苗族瑶族傣族自治县\",\"彝族回族苗族自治县\",\"满族蒙古族自治县\",\"土家族苗族自治州\",\"哈尼族彝族自治县\",\n",
    "        \"布依族苗族自治州\",\"蒙古族藏族自治州\",\"哈尼族彝族自治州\",\"傣族景颇族自治州\",\"土家族苗族自治县\",\"白族普米族自治县\",\n",
    "        \"苗族土家族自治县\",\"布依族苗族自治县\",\"土家族苗族自治县\",\"苗族布依族自治县\",\"仡佬族苗族自治县\",\"哈尼族彝族自治县\",\n",
    "        \"独龙族怒族自治县\",\"壮族苗族自治州\",\"苗族侗族自治州\",\"黎族苗族自治县\",\"壮族瑶族自治县\",\"藏族羌族自治州\",\n",
    "        \"苗族侗族自治县\",\"回族苗族自治县\",\"彝族傣族自治县\",\"彝族傣族自治县\",\"彝族苗族自治县\",\"傣族佤族自治县\",\"柯尔克孜自治州\",\n",
    "        \"苗族彝族自治县\",\"回族彝族自治县\",\"回族土族自治县\",\"彝族回族自治县\",\"族撒拉族自治县\",\"哈萨克族自治县\",\"哈萨克自治州\",\n",
    "        \"拉祜族自治县\",\"纳西族自治县\",\"哈尼族自治县\",\"傈僳族自治县\",\"哈萨克自治县\",\"仫佬族自治县\",\"毛南族自治县\",\n",
    "        \"傈僳族自治州\",\"朝鲜族自治县\",\"朝鲜族自治州\",\"蒙古族自治县\",\"撒拉族自治县\",\"土家族自治县\",\"维吾尔自治区\",\n",
    "        \"各族自治县\",\"佤族自治县\",\"回族自治县\",\"土族自治县\",\"彝族自治县\",\"锡伯自治县\",\"水族自治县\",\"怒族自治县\",\"羌族自治县\",\n",
    "        \"藏族自治县\",\"彝族自治县\",\"苗族自治县\",\"侗族自治县\",\"瑶族自治县\",\"满族自治县\",\"回族自治县\",\"蒙古自治州\",\"白族自治州\",\n",
    "        \"固族自治县\",\"瑶族自治县\",\"傣族自治州\",\"苗族自治县\",\"藏族自治州\",\"畲族自治县\",\"彝族自治州\",\"回族自治州\",\"壮族自治区\",\n",
    "        \"黎族自治县\",\"锡伯自治县\",\"特别行政区\",\"地区\",\"自治区\",\"自治县\",\"省\",\"市\",\"盟\",\"县\",\"区\",\"镇\",\"乡\",\"村\"]\n",
    "    special_location = [\"神农架林区\"]\n",
    "    for x in p_c_t:\n",
    "        if location == \"神农架林区\":\n",
    "            location = \"神农架\"\n",
    "        elif len(location) > 2 and x in location:\n",
    "            location = location[:-len(x)]\n",
    "            break\n",
    "    return location\n",
    "# 读取地名数据\n",
    "import json\n",
    "def read_location(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        china_location_data = json.load(file)\n",
    "    locations_province_list = []\n",
    "    locations_city_list = []\n",
    "    locations_area_list = []\n",
    "    \n",
    "    for p_i in range(0, len(china_location_data)):\n",
    "        province = remove_p_c_a(china_location_data[p_i]['name'])\n",
    "        locations_province_list.append(province)\n",
    "        for c_i in range(0, len(china_location_data[p_i]['city'])):\n",
    "            city = remove_p_c_a(china_location_data[p_i]['city'][c_i]['name'])\n",
    "            locations_city_list.append(city)\n",
    "            for a_i in range(0, len(china_location_data[p_i]['city'][c_i]['area'])):\n",
    "                area = remove_p_c_a(china_location_data[p_i]['city'][c_i]['area'][a_i])\n",
    "                locations_area_list.append(area)\n",
    "\n",
    "    return locations_province_list, locations_city_list, locations_area_list\n",
    "\n",
    "# 尝试发现同音地名，写入文件\n",
    "from pypinyin import lazy_pinyin\n",
    "def discover_homophone_locations(locations_list):\n",
    "    homophone_locations_list = []\n",
    "    for i in range(0, len(locations_list)):\n",
    "        if i < len(locations_list) - 1:\n",
    "            for j in range (i + 1, len(locations_list)):\n",
    "                if lazy_pinyin(locations_list[i]) == lazy_pinyin(locations_list[j]):\n",
    "                    homophone_locations = locations_list[i] + \"\\t\" + locations_list[j]\n",
    "                    homophone_locations_list.append(homophone_locations)\n",
    "        if i % 250 == 0:\n",
    "            print(\"%.2f of locations for homophone have processed \" %((i/len(locations_list))*100))    \n",
    "    write_file(\"common_data/homophone_locations.txt\",homophone_locations_list)\n",
    "    return homophone_locations_list\n",
    "\n",
    "# 载入并读取地点名称的json文件，返回 provinces_list, citys_list, areas_list\n",
    "location_file_path = 'common_data/locations.json'   \n",
    "provinces_list, citys_list, areas_list = read_location(location_file_path)\n",
    "print(len(provinces_list))\n",
    "print(len(citys_list))\n",
    "print(len(areas_list))\n",
    "\n",
    "# 将所有地名合并到locations里\n",
    "locations = list(set(provinces_list + citys_list + areas_list))\n",
    "print(len(locations))\n",
    "print(locations[1000])\n",
    "\n",
    "# 尝试发现同音地名\n",
    "homophone_locations_list = discover_homophone_locations(locations)\n",
    "print(len(homophone_locations_list))\n",
    "print(homophone_locations_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:26:55.756101Z",
     "iopub.status.busy": "2024-10-14T03:26:55.755191Z",
     "iopub.status.idle": "2024-10-14T03:27:00.569668Z",
     "shell.execute_reply": "2024-10-14T03:27:00.568787Z",
     "shell.execute_reply.started": "2024-10-14T03:26:55.756064Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The number of lines in common_data/homophone_locations.txt is 325\r\n",
      "325\r\n",
      "('大观', '大关')\r\n",
      "\r\n",
      "The number of lines in work/test_B_processed.txt is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "=============== In Progress ===============\r\n",
      "10.00 of text pairs for different subsets identification have processed \r\n",
      "20.00 of text pairs for different subsets identification have processed \r\n",
      "30.00 of text pairs for different subsets identification have processed \r\n",
      "40.00 of text pairs for different subsets identification have processed \r\n",
      "50.00 of text pairs for different subsets identification have processed \r\n",
      "60.00 of text pairs for different subsets identification have processed \r\n",
      "70.00 of text pairs for different subsets identification have processed \r\n",
      "80.00 of text pairs for different subsets identification have processed \r\n",
      "90.00 of text pairs for different subsets identification have processed \r\n",
      "100.00 of text pairs for different subsets identification have processed \r\n",
      "===============     Done    ===============\r\n",
      "0\r\n",
      "3\r\n",
      "句子对: 山西应急厅厅长\t陕西应急厅厅长\r\n",
      "子串: 3306\t0\r\n",
      "句子对: 陕西和四川有多远\t山西和四川有多远\r\n",
      "子串: 59869\t0\r\n",
      "句子对: 山西都有哪些县\t陕西都有哪些县\r\n",
      "子串: 67327\t0\r\n"
     ]
    }
   ],
   "source": [
    "# 同音地名不作为错误纠正\n",
    "# 读取同音地名文件，存入list\n",
    "def read_homophone_locations(file_path):\n",
    "    homophone_locations_list = []\n",
    "    text_1, text_2 = read_candidates_test(file_path)\n",
    "    for i in range(0, len(text_1)):\n",
    "        homophone_locations_list.append((text_1[i],text_2[i]))\n",
    "    return homophone_locations_list\n",
    "    \n",
    "# 从文件中读取\n",
    "homophone_locations_list = read_homophone_locations(\"common_data/homophone_locations.txt\")\n",
    "print(len(homophone_locations_list))\n",
    "print(homophone_locations_list[0])\n",
    "\n",
    "# 同音地名不作为错误纠正\n",
    "exclusion_list = list(set(homophone_locations_list)) \n",
    "\n",
    "# 将带有exclusion list中 字词的句子对找出\n",
    "def discover_exclusion(exclusion_list, text_list_1, text_list_2, text_label_list, label_value = \"0\", print_progress = False):\n",
    "    exclusion_text_pair_0_list = []\n",
    "    exclusion_label_0_list = []\n",
    "    exclusion_text_pair_1_list = []\n",
    "    exclusion_label_1_list = []\n",
    "    count_percent = 0\n",
    "\n",
    "    print(\"=\"*15 + \" In Progress \" + \"=\"*15)\n",
    "    for i in range (0, len(text_list_1)):\n",
    "        count_percent = count_percent + 1\n",
    "        text_1 = text_list_1[i]\n",
    "        text_2 = text_list_2[i]\n",
    "    \n",
    "        for exclusion in exclusion_list:\n",
    "            if (len(text_1) == len(text_2)):\n",
    "                if (exclusion[0] in text_1 and exclusion[1] in text_2) or (exclusion[0] in text_2 and exclusion[1] in text_1):\n",
    "                    if (text_1.replace(exclusion[0],\"\") == text_2.replace(exclusion[1],\"\")) or (text_2.replace(exclusion[0],\"\") == text_1.replace(exclusion[1],\"\")):\n",
    "                        if label_value == \"0\":\n",
    "                            exclusion_text_pair_0_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                            if text_label_list!= None:\n",
    "                                exclusion_label_0_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "                            break\n",
    "                        else:\n",
    "                            exclusion_text_pair_1_list.append(text_list_1[i] + \"\\t\" + text_list_2[i])\n",
    "                            if text_label_list!= None:\n",
    "                                exclusion_label_1_list.append(str(i) + \"\\t\" + str(text_label_list[i]))\n",
    "                            break\n",
    "\n",
    "        if print_progress and count_percent%10000 == 0:\n",
    "            print(\"%.2f of text pairs for different subsets identification have processed \" %((count_percent/len(text_list_1))*100))    \n",
    "    print(\"=\"*15 + \"     Done    \" + \"=\"*15)\n",
    "    return exclusion_text_pair_1_list, exclusion_label_1_list, exclusion_text_pair_0_list, exclusion_label_0_list\n",
    "\n",
    "# 使用 准备数据\n",
    "test_text_pair_1_list,test_text_pair_2_list = read_candidates_test(\"work/test_B_processed.txt\")\n",
    "view_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\") #仅仅用来查看，不影响结果\n",
    "\n",
    "exclusion_text_pair_1_list, exclusion_label_1_list, exclusion_text_pair_0_list, exclusion_label_0_list = discover_exclusion(exclusion_list, test_text_pair_1_list,test_text_pair_2_list, view_label_list, \"0\",  True)\n",
    "write_file(\"common_data/exclusion_1_list.txt\",exclusion_text_pair_1_list)\n",
    "write_file(\"common_data/exclusion_0_list.txt\",exclusion_text_pair_0_list)\n",
    "\n",
    "#查看文件中的句子对数量\n",
    "print(len(read_candidates_line(\"common_data/exclusion_1_list.txt\")))\n",
    "print_examples(exclusion_text_pair_1_list,exclusion_label_1_list,None, 100)\n",
    "print(len(read_candidates_line(\"common_data/exclusion_0_list.txt\")))\n",
    "print_examples(exclusion_text_pair_0_list,exclusion_label_0_list,None, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.7 校正Misspelling预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:27:08.948516Z",
     "iopub.status.busy": "2024-10-14T03:27:08.947889Z",
     "iopub.status.idle": "2024-10-14T03:27:08.955814Z",
     "shell.execute_reply": "2024-10-14T03:27:08.955017Z",
     "shell.execute_reply.started": "2024-10-14T03:27:08.948473Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 依据输入的列表校正预测值\n",
    "def adjust_predict_label(test_a_text_pairs, final_predict_lables, index_and_adjusted_labels, print_correction = True):\n",
    "    print(\"test_a_text_pairs: \" + str(len(test_a_text_pairs)))\n",
    "    print(\"final_predict_lables\" + str(len(final_predict_lables)))\n",
    "    print(\"index_and_adjusted_labels\" + str(len(index_and_adjusted_labels)))\n",
    "    count = 0\n",
    "    for i in range(0, len(index_and_adjusted_labels)):\n",
    "        index_label = index_and_adjusted_labels[i].rstrip().split(\"\\t\")\n",
    "        index = int(index_label[0])\n",
    "        adjusted_label = index_label[1]\n",
    "        if final_predict_lables[index]!= adjusted_label:\n",
    "            count = count + 1\n",
    "            if print_correction:\n",
    "                print(test_a_text_pairs[index])\n",
    "                print('index:' + str(index))\n",
    "                print('predict_label:' + str(final_predict_lables[index]))\n",
    "                print('adjusted_label:' + str(adjusted_label))\n",
    "                print(\"=\"*30)\n",
    "            final_predict_lables[index] = adjusted_label \n",
    "    print (\"预测值校正个数为: \" + str(count))\n",
    "    return final_predict_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:27:15.838493Z",
     "iopub.status.busy": "2024-10-14T03:27:15.837820Z",
     "iopub.status.idle": "2024-10-14T03:27:25.797166Z",
     "shell.execute_reply": "2024-10-14T03:27:25.796238Z",
     "shell.execute_reply.started": "2024-10-14T03:27:15.838434Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_a_data_line_list: 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "predict_label_list: 100000\r\n",
      "6417\r\n",
      "403\r\n",
      "misspelling_index_label_list: 7650\r\n",
      "test_a_text_pairs: 100000\r\n",
      "final_predict_lables100000\r\n",
      "index_and_adjusted_labels7650\r\n",
      "章子怡是那里人\t张子怡是那里人\r\n",
      "index:2284\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "王子怡是谁\t汪子怡是谁\r\n",
      "index:4449\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "乌兰图雅的老公是谁\t乌蓝图雅的老公是谁\r\n",
      "index:6510\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "伊可尔治疗扁平疣\t伊可儿治疗扁平疣\r\n",
      "index:6982\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "程咬金是什么生肖\t程咬何是什么生肖\r\n",
      "index:14676\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "刘亚的拼音\t刘娅的拼音\r\n",
      "index:19050\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "大空翼是哪个球队的\t大平翼是哪个球队的\r\n",
      "index:19052\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "刘晓燕哪里人\t刘小燕哪里人\r\n",
      "index:21532\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "什么玛丽亚\t什么玛利亚\r\n",
      "index:27038\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "武则天原籍是哪里\t常则天原籍是哪里\r\n",
      "index:30955\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "刘恺威多少岁数\t刘凯威多少岁数\r\n",
      "index:32870\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "金三顺作品\t金叁顺作品\r\n",
      "index:33513\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "朱元漳为什么要大杀功臣\t朱元璋为什么要大杀功臣\r\n",
      "index:34693\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "贾玲多少岁了\t嘉玲多少岁了\r\n",
      "index:37070\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "张子枫多大\t章子枫多大\r\n",
      "index:39366\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "王健林哪里人\t王建林哪里人\r\n",
      "index:49319\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "余秋雨的作品\t于秋雨的作品\r\n",
      "index:49334\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "韩语姐姐怎么说\t汉语姐姐怎么说\r\n",
      "index:56723\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "杨幂和刘恺威真的复合了吗\t杨幂和刘凯威真的复合了吗\r\n",
      "index:56730\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "张子枫多大\t张子风多大\r\n",
      "index:59650\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "唐嫣哪里人\t唐岩哪里人\r\n",
      "index:65984\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "朱德是那里人\t朱徳是那里人\r\n",
      "index:66374\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "萨达姆是哪国的\t撒达姆是哪国的\r\n",
      "index:75613\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "陈薇院士研究的疫苗属于哪类疫苗\t陈微院士研究的疫苗属于哪类疫苗\r\n",
      "index:80043\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "最强nba钻石艾弗森怎么加点\t最强nba钻石艾佛森怎么加点\r\n",
      "index:82244\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "刘昊然演的综艺\t刘浩然演的综艺\r\n",
      "index:86822\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "叶绍翁是哪个朝代的\t叶少翁是哪个朝代的\r\n",
      "index:99232\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "你知道是\t你知道是\r\n",
      "index:1111\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "ar镀膜玻璃的\tar镀膜玻璃的\r\n",
      "index:1297\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "吃什么养胃\t吃十么养胃\r\n",
      "index:1780\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "章子怡是那里人\t张子怡是那里人\r\n",
      "index:2284\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "我要运动\t我要运动\r\n",
      "index:2523\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "山西应急厅厅长\t陕西应急厅厅长\r\n",
      "index:3306\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "oppowatch防水\toppowatch防水\r\n",
      "index:4164\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "王子怡是谁\t汪子怡是谁\r\n",
      "index:4449\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "乌兰图雅的老公是谁\t乌蓝图雅的老公是谁\r\n",
      "index:6510\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "大连大学排名\t大连大学排名\r\n",
      "index:6908\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "伊可尔治疗扁平疣\t伊可儿治疗扁平疣\r\n",
      "index:6982\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "能组什么词\t能组什么词\r\n",
      "index:7671\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "主动的动字怎么写\t自动的动字怎么写\r\n",
      "index:8119\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "月经来之前乳房疼怎么回事\t月经来之前乳房痒怎么回事\r\n",
      "index:13024\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "药品怎么读\t药品怎么卖\r\n",
      "index:13355\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "咱们聪明\t咱们聪明\r\n",
      "index:13375\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "心脏不好吃什么\t心脏不好吃什药\r\n",
      "index:14951\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "玩一会儿\t玩一会儿\r\n",
      "index:15289\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "刘亚的拼音\t刘娅的拼音\r\n",
      "index:19050\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "冀州在哪个省\t蓟州在哪个省\r\n",
      "index:21043\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "能组什么词\t能组什么词\r\n",
      "index:22085\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "用英语怎么读\t用英语怎么读\r\n",
      "index:23080\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "你没有眼睛\t你没有眼睛\r\n",
      "index:23769\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "偏头痛是怎么引起的\t遍头痛是怎么引起的\r\n",
      "index:24565\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "什么是世界文化遗产\t十么是世界文化遗产\r\n",
      "index:24628\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "用英语怎么说\t用英语怎么说\r\n",
      "index:25038\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "还没回来\t还没回来\r\n",
      "index:25984\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "什么玛丽亚\t什么玛利亚\r\n",
      "index:27038\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "怎么样的\t怎么样的\r\n",
      "index:27621\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "教学设计过程模式\t教学设计过程模式\r\n",
      "index:28158\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "你有脑袋\t你有脑袋\r\n",
      "index:28972\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "形容珍惜的\t形容珍惜的\r\n",
      "index:30076\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "没有网络\t没有网络\r\n",
      "index:30886\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "oppocare加的退换规则\toppocare加的退换规则\r\n",
      "index:30900\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "配件英语\t备件英语\r\n",
      "index:32868\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "广播电视播音员主持人资格证\t广播电视播音员主持人资格证\r\n",
      "index:33034\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "金三顺作品\t金叁顺作品\r\n",
      "index:33513\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "朱元漳为什么要大杀功臣\t朱元璋为什么要大杀功臣\r\n",
      "index:34693\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "什么的翅膀\t什么的翅胖\r\n",
      "index:35160\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "肝不好吃什么\t肝不好吃什药\r\n",
      "index:38152\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "零除零\t零除零\r\n",
      "index:39053\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "张子枫多大\t章子枫多大\r\n",
      "index:39366\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "落枕脖子疼怎么办\t洛枕脖子疼怎么办\r\n",
      "index:39739\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "需要刷机\t需要刷机\r\n",
      "index:43146\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "脸上皮肤痒是怎么回事\t脸上皮肤庠是怎么回事\r\n",
      "index:43468\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "癣会传染吗\t显会传染吗\r\n",
      "index:46709\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "双麦降噪的\t双麦降噪的\r\n",
      "index:47383\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "用日语说\t用日语说\r\n",
      "index:47496\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "氯化钠是不是生理盐水\t录化钠是不是生理盐水\r\n",
      "index:47905\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "王健林哪里人\t王建林哪里人\r\n",
      "index:49319\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "余秋雨的作品\t于秋雨的作品\r\n",
      "index:49334\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "我叫什么\t我叫什么\r\n",
      "index:50614\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "换电话微信怎么恢复\t换电话微信怎么恢复\r\n",
      "index:51953\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "杨幂和刘恺威真的复合了吗\t杨幂和刘凯威真的复合了吗\r\n",
      "index:56730\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "农行信用卡怎么提前还款\t农行信用卡怎么提前还款\r\n",
      "index:59567\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "张子枫多大\t张子风多大\r\n",
      "index:59650\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "陈字偏旁是什么\t陈字边旁是什么\r\n",
      "index:59741\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "陕西和四川有多远\t山西和四川有多远\r\n",
      "index:59869\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "了解的英文\t了解的英文\r\n",
      "index:61715\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "怀孕mt检查\t怀孕nt检查\r\n",
      "index:61756\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "李艺泽这个名字怎么样\t李雨泽这个名字怎么样\r\n",
      "index:62332\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "么打电话\t么打电话\r\n",
      "index:62865\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "身份证号码怎么\t身份证号码怎么\r\n",
      "index:64766\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "帮我找找\t帮我找找\r\n",
      "index:65619\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "内眼角痒是怎么回事\t内眼角庠是怎么回事\r\n",
      "index:65712\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "一千零八十p高清画质的\t一千零八十p高清画质的\r\n",
      "index:66241\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "快手怎么查看别人\t快手怎么查看别人\r\n",
      "index:66878\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "山西都有哪些县\t陕西都有哪些县\r\n",
      "index:67327\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "嗓子疼喝什么好\t嗓子痒喝什么好\r\n",
      "index:67545\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "日文怎么说\t日文怎么说\r\n",
      "index:68609\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "什么都可以\t什么都可以\r\n",
      "index:68686\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "撒的多音字组字\t散的多音字组字\r\n",
      "index:68807\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "脖子疼怎么缓解\t脖子痒怎么缓解\r\n",
      "index:70930\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "素菜有那些\t蔬菜有那些\r\n",
      "index:71348\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "五分加五分\t五分加五分\r\n",
      "index:71517\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "耳朵痒是怎么回事\t耳朵庠是怎么回事\r\n",
      "index:71521\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "mba报名时间\tmpa报名时间\r\n",
      "index:71526\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "怀孕能吃火鸡\t怀孕能吃火鸡\r\n",
      "index:72236\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "我普通话不标准\t我普通话不标准\r\n",
      "index:73419\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "萨达姆是哪国的\t撒达姆是哪国的\r\n",
      "index:75613\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "打开快手\t打开快手\r\n",
      "index:76628\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "怎么刷白皮鞋\t怎么刷白布鞋\r\n",
      "index:76636\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "我的心在哪里\t我的心在哪里\r\n",
      "index:76682\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "你会长大\t你会长大\r\n",
      "index:76972\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "什么时候开学\t十么时候开学\r\n",
      "index:77903\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "新冠状病毒的症状头痛\t新冠状病毒的症状头痛\r\n",
      "index:79798\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "陈薇院士研究的疫苗属于哪类疫苗\t陈微院士研究的疫苗属于哪类疫苗\r\n",
      "index:80043\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "你会吹牛逼\t你会吹牛逼\r\n",
      "index:80405\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "孙红雷的电视剧\t孙红梅的电视剧\r\n",
      "index:80568\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "晨曦名字的含义是什么\t晨旭名字的含义是什么\r\n",
      "index:81221\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "如何报考mba\t如何报考mpa\r\n",
      "index:82043\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "最强nba钻石艾弗森怎么加点\t最强nba钻石艾佛森怎么加点\r\n",
      "index:82244\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "什么有助于睡眠\t十么有助于睡眠\r\n",
      "index:84519\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "刘昊然演的综艺\t刘浩然演的综艺\r\n",
      "index:86822\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "你智力好\t你智力好\r\n",
      "index:90811\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "成语接龙\t成语接龙\r\n",
      "index:91684\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "零除零\t零除零\r\n",
      "index:95977\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "会不会学猫叫\t会不会学猫叫\r\n",
      "index:96223\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "叶绍翁是哪个朝代的\t叶少翁是哪个朝代的\r\n",
      "index:99232\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "明宣宗之后是哪个皇帝\t明宪宗之后是哪个皇帝\r\n",
      "index:99339\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "你有爱人\t你有爱人\r\n",
      "index:99738\r\n",
      "predict_label:0\r\n",
      "adjusted_label:1\r\n",
      "==============================\r\n",
      "山西应急厅厅长\t陕西应急厅厅长\r\n",
      "index:3306\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "陕西和四川有多远\t山西和四川有多远\r\n",
      "index:59869\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "山西都有哪些县\t陕西都有哪些县\r\n",
      "index:67327\r\n",
      "predict_label:1\r\n",
      "adjusted_label:0\r\n",
      "==============================\r\n",
      "预测值校正个数为: 131\r\n"
     ]
    }
   ],
   "source": [
    "# test a text pairs and predict labels\n",
    "test_a_data_line_list = read_candidates_line(\"work/test_B_processed.txt\")\n",
    "print(\"test_a_data_line_list: \" + str(len(test_a_data_line_list)))\n",
    "\n",
    "predict_label_list = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\")\n",
    "print(\"predict_label_list: \" + str(len(predict_label_list)))\n",
    "\n",
    "# homephone + pycorrector 读取句子对\n",
    "pinyin_1_list = read_candidates_line(\"common_data/pinyin_1_text.txt\")\n",
    "pinyin_0_list = read_candidates_line(\"common_data/pinyin_0_text.txt\")\n",
    "pycorrector_1_list = read_candidates_line(\"common_data/pycorrector_1_list.txt\")\n",
    "structure_1_list = read_candidates_line(\"common_data/structure_1_list.txt\")\n",
    "name_1_list = read_candidates_line(\"common_data/name_1_list.txt\")\n",
    "name_0_list = read_candidates_line(\"common_data/name_0_list.txt\")\n",
    "equal_1_list = read_candidates_line(\"common_data/equal_1_list.txt\")\n",
    "\n",
    "misspelling_1_list = list(set(pinyin_1_list + pycorrector_1_list + structure_1_list + name_1_list + equal_1_list))\n",
    "misspelling_0_list = list(set(pinyin_0_list + name_0_list))\n",
    "exclusion_1_list = read_candidates_line(\"common_data/exclusion_1_list.txt\")\n",
    "exclusion_0_list = read_candidates_line(\"common_data/exclusion_0_list.txt\")\n",
    "\n",
    "print(len(misspelling_1_list))\n",
    "print(len(misspelling_0_list))\n",
    "\n",
    "# 已校正的句子对在后续工作中无需重复处理\n",
    "final_misspelling_list = list(set(misspelling_1_list + misspelling_0_list + exclusion_1_list + exclusion_0_list))\n",
    "write_file(\"common_data/misspelling_list.txt\",final_misspelling_list )\n",
    "\n",
    "# 赋值\n",
    "misspelling_1_index_label_list = []\n",
    "for i in range (0, len(test_a_data_line_list)):\n",
    "    if test_a_data_line_list[i] in misspelling_1_list:\n",
    "        misspelling_1_index_label_list.append(str(i)+\"\\t1\")\n",
    "\n",
    "misspelling_0_index_label_list = []\n",
    "for i in range (0, len(test_a_data_line_list)):\n",
    "    if test_a_data_line_list[i] in misspelling_0_list:\n",
    "        misspelling_0_index_label_list.append(str(i)+\"\\t0\")\n",
    "\n",
    "exclusion_0_index_label_list = []\n",
    "for i in range (0, len(test_a_data_line_list)):\n",
    "    if test_a_data_line_list[i] in exclusion_0_list:\n",
    "        exclusion_0_index_label_list.append(str(i)+\"\\t0\")\n",
    "\n",
    "exclusion_1_index_label_list = []\n",
    "for i in range (0, len(test_a_data_line_list)):\n",
    "    if test_a_data_line_list[i] in exclusion_1_list:\n",
    "        exclusion_1_index_label_list.append(str(i)+\"\\t1\")\n",
    "\n",
    "# 先后顺序很重要\n",
    "misspelling_index_label_list =  misspelling_0_index_label_list + misspelling_1_index_label_list + exclusion_0_index_label_list + exclusion_1_index_label_list\n",
    "\n",
    "print(\"misspelling_index_label_list: \" + str(len(misspelling_index_label_list)))\n",
    "\n",
    "# 使用 方法4-1 来 校正 预测值 \n",
    "final_predict_lables = adjust_predict_label(test_a_data_line_list, predict_label_list, misspelling_index_label_list, True)\n",
    "\n",
    "# 写入文件\n",
    "write_file(\"predict_results/ms_ccf_qianyan_qm_result_B.csv\",final_predict_lables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 对比校正结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:27:34.392919Z",
     "iopub.status.busy": "2024-10-14T03:27:34.392286Z",
     "iopub.status.idle": "2024-10-14T03:27:34.577320Z",
     "shell.execute_reply": "2024-10-14T03:27:34.576354Z",
     "shell.execute_reply.started": "2024-10-14T03:27:34.392882Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The number of lines in work/test_B_1118.tsv is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "\r\n",
      "The number of predict result lines in predict_results/ms_ccf_qianyan_qm_result_B.csv is 100000\r\n",
      "100000\r\n",
      "100000\r\n",
      "100000\r\n",
      "100000\r\n",
      "No. 1111 label 1 = 0 lable 2 = 1 text pair: 你知道是吗 你知道是\r\n",
      "No. 1297 label 1 = 0 lable 2 = 1 text pair: AR镀膜玻璃的介绍 ar镀膜玻璃的\r\n",
      "No. 1780 label 1 = 0 lable 2 = 1 text pair: 吃什么养胃 吃十么养胃\r\n",
      "No. 2523 label 1 = 0 lable 2 = 1 text pair: 我要运动吗 我要运动\r\n",
      "No. 4164 label 1 = 0 lable 2 = 1 text pair: OPPOWATCH防水吗 OPPOWATCH防水\r\n",
      "No. 6908 label 1 = 0 lable 2 = 1 text pair: 大连市大学排名 大连大学排名\r\n",
      "No. 7671 label 1 = 0 lable 2 = 1 text pair: 呵能组什么词 能组什么词\r\n",
      "No. 8119 label 1 = 0 lable 2 = 1 text pair: 主动的动字怎么写 自动的动字怎么写\r\n",
      "No. 13024 label 1 = 0 lable 2 = 1 text pair: 月经来之前乳房疼怎么回事 月经来之前乳房痒怎么回事\r\n",
      "No. 13355 label 1 = 0 lable 2 = 1 text pair: 药品怎么读 药品怎么卖\r\n",
      "No. 13375 label 1 = 0 lable 2 = 1 text pair: 咱们聪明吗 咱们聪明啊\r\n",
      "No. 14676 label 1 = 1 lable 2 = 0 text pair: 程咬金是什么生肖 程咬何是什么生肖\r\n",
      "No. 14951 label 1 = 0 lable 2 = 1 text pair: 心脏不好吃什么 心脏不好吃什药\r\n",
      "No. 15289 label 1 = 0 lable 2 = 1 text pair: 玩一会儿吗 玩一会儿啊\r\n",
      "No. 19052 label 1 = 1 lable 2 = 0 text pair: 大空翼是哪个球队的？？？ 大平翼是哪个球队的？？？\r\n",
      "No. 21043 label 1 = 0 lable 2 = 1 text pair: 冀州在哪个省 蓟州在哪个省\r\n",
      "No. 21532 label 1 = 1 lable 2 = 0 text pair: 刘晓燕哪里人 刘小燕哪里人\r\n",
      "No. 22085 label 1 = 0 lable 2 = 1 text pair: 嘿能组什么词 能组什么词\r\n",
      "No. 23080 label 1 = 0 lable 2 = 1 text pair: 用英语怎么读 知道用英语怎么读\r\n",
      "No. 23769 label 1 = 0 lable 2 = 1 text pair: 你没有眼睛吗 你没有眼睛呀\r\n",
      "No. 24565 label 1 = 0 lable 2 = 1 text pair: 偏头痛是怎么引起的 遍头痛是怎么引起的\r\n",
      "No. 24628 label 1 = 0 lable 2 = 1 text pair: 什么是世界文化遗产 十么是世界文化遗产\r\n",
      "No. 25038 label 1 = 0 lable 2 = 1 text pair: 嗯用英语怎么说 用英语怎么说\r\n",
      "No. 25984 label 1 = 0 lable 2 = 1 text pair: 还没回来吗 还没回来呀\r\n",
      "No. 27621 label 1 = 0 lable 2 = 1 text pair: 怎么样的吗 怎么样的呀\r\n",
      "No. 28158 label 1 = 0 lable 2 = 1 text pair: 简答教学设计过程模式 教学设计过程模式\r\n",
      "No. 28972 label 1 = 0 lable 2 = 1 text pair: 你有脑袋吗 你有脑袋呢\r\n",
      "No. 30076 label 1 = 0 lable 2 = 1 text pair: 形容珍惜的说说 形容珍惜的\r\n",
      "No. 30886 label 1 = 0 lable 2 = 1 text pair: 没有网络吗 没有网络啊\r\n",
      "No. 30900 label 1 = 0 lable 2 = 1 text pair: OPPOCare+的退换规则 OPPO Care+的退换规则\r\n",
      "No. 30955 label 1 = 1 lable 2 = 0 text pair: 武则天原籍是哪里？ 常则天原籍是哪里？\r\n",
      "No. 32868 label 1 = 0 lable 2 = 1 text pair: 配件 英语 备件 英语\r\n",
      "No. 32870 label 1 = 1 lable 2 = 0 text pair: 刘恺威多少岁数 刘凯威多少岁数\r\n",
      "No. 33034 label 1 = 0 lable 2 = 1 text pair: 广播电视播音员主持人资格证 广播电视播音员主持人资格证回答\r\n",
      "No. 35160 label 1 = 0 lable 2 = 1 text pair: 什么的翅膀 什么的翅胖\r\n",
      "No. 37070 label 1 = 1 lable 2 = 0 text pair: 贾玲多少岁了 嘉玲多少岁了\r\n",
      "No. 38152 label 1 = 0 lable 2 = 1 text pair: 肝不好吃什么 肝不好吃什药\r\n",
      "No. 39053 label 1 = 0 lable 2 = 1 text pair: 0÷0等于多少 0除以0等于几\r\n",
      "No. 39739 label 1 = 0 lable 2 = 1 text pair: 落枕脖子疼怎么办 洛枕脖子疼怎么办\r\n",
      "No. 43146 label 1 = 0 lable 2 = 1 text pair: 需要刷机 需要刷机吗\r\n",
      "No. 43468 label 1 = 0 lable 2 = 1 text pair: 脸上皮肤痒是怎么回事 脸上皮肤庠是怎么回事\r\n",
      "No. 46709 label 1 = 0 lable 2 = 1 text pair: 癣会传染吗 显会传染吗\r\n",
      "No. 47383 label 1 = 0 lable 2 = 1 text pair: 双麦降噪的介绍 双麦降噪的\r\n",
      "No. 47496 label 1 = 0 lable 2 = 1 text pair: 用日语说 用日语说你好\r\n",
      "No. 47905 label 1 = 0 lable 2 = 1 text pair: 氯化钠是不是生理盐水 录化钠是不是生理盐水\r\n",
      "No. 50614 label 1 = 0 lable 2 = 1 text pair: 知道我叫什么 请问我叫什么\r\n",
      "No. 51953 label 1 = 0 lable 2 = 1 text pair: 换电话微信怎么恢复 换电话微信怎么恢复我一个朋友\r\n",
      "No. 56723 label 1 = 1 lable 2 = 0 text pair: 韩语姐姐怎么说 汉语姐姐怎么说\r\n",
      "No. 59567 label 1 = 0 lable 2 = 1 text pair: 农行信用卡怎么提前还款 农业银行信用卡怎么提前还款\r\n",
      "No. 59741 label 1 = 0 lable 2 = 1 text pair: 陈字偏旁是什么 陈字边旁是什么\r\n",
      "No. 61715 label 1 = 0 lable 2 = 1 text pair: 了解的英文 知道了解的英文\r\n",
      "No. 61756 label 1 = 0 lable 2 = 1 text pair: 怀孕 mt检查 怀孕 nt 检查\r\n",
      "No. 62332 label 1 = 0 lable 2 = 1 text pair: 李艺泽这个名字怎么样 李雨泽这个名字怎么样\r\n",
      "No. 62865 label 1 = 0 lable 2 = 1 text pair: 么么打电话 那么打电话\r\n",
      "No. 64766 label 1 = 0 lable 2 = 1 text pair: 身份证号码怎么解释 身份证号码怎么\r\n",
      "No. 65619 label 1 = 0 lable 2 = 1 text pair: 帮我找找吗 帮我找找谢谢\r\n",
      "No. 65712 label 1 = 0 lable 2 = 1 text pair: 内眼角痒是怎么回事 内眼角庠是怎么回事\r\n",
      "No. 65984 label 1 = 1 lable 2 = 0 text pair: 唐嫣哪里人 唐岩哪里人\r\n",
      "No. 66241 label 1 = 0 lable 2 = 1 text pair: 1080P高清画质的介绍 1080p高清画质的\r\n",
      "No. 66374 label 1 = 1 lable 2 = 0 text pair: 朱德是那里人 朱徳是那里人\r\n",
      "No. 66878 label 1 = 0 lable 2 = 1 text pair: 快手怎么查看别人说说 快手怎么查看别人\r\n",
      "No. 67545 label 1 = 0 lable 2 = 1 text pair: 嗓子疼喝什么好 嗓子痒喝什么好\r\n",
      "No. 68609 label 1 = 0 lable 2 = 1 text pair: 你好日文怎么说 日文怎么说\r\n",
      "No. 68686 label 1 = 0 lable 2 = 1 text pair: 你们说什么都可以 什么都可以吗\r\n",
      "No. 68807 label 1 = 0 lable 2 = 1 text pair: 撒的多音字组字 散的多音字组字\r\n",
      "No. 70930 label 1 = 0 lable 2 = 1 text pair: 脖子疼怎么缓解 脖子痒怎么缓解\r\n",
      "No. 71348 label 1 = 0 lable 2 = 1 text pair: 素菜有那些 蔬菜有那些\r\n",
      "No. 71517 label 1 = 0 lable 2 = 1 text pair: 五分+五分等于多少 5分+5分等于多少\r\n",
      "No. 71521 label 1 = 0 lable 2 = 1 text pair: 耳朵痒是怎么回事 耳朵庠是怎么回事\r\n",
      "No. 71526 label 1 = 0 lable 2 = 1 text pair: mba 报名 时间 mpa报名时间\r\n",
      "No. 72236 label 1 = 0 lable 2 = 1 text pair: 怀孕能吃火鸡 怀孕能吃火鸡吗\r\n",
      "No. 73419 label 1 = 0 lable 2 = 1 text pair: 我普通话不标准 我普通话不标准吗\r\n",
      "No. 76628 label 1 = 0 lable 2 = 1 text pair: 打开快手吗 打开快手谢谢\r\n",
      "No. 76636 label 1 = 0 lable 2 = 1 text pair: 怎么刷白皮鞋 怎么刷白布鞋\r\n",
      "No. 76682 label 1 = 0 lable 2 = 1 text pair: 我为什么想知道我的心在哪里 我的心在哪里\r\n",
      "No. 76972 label 1 = 0 lable 2 = 1 text pair: 你会长大吗 你会长大\r\n",
      "No. 77903 label 1 = 0 lable 2 = 1 text pair: 什么时候开学 十么时候开学\r\n",
      "No. 79798 label 1 = 0 lable 2 = 1 text pair: 新冠状病毒的症状头痛 新冠状病毒的症状头痛吗\r\n",
      "No. 80405 label 1 = 0 lable 2 = 1 text pair: 你会吹牛逼吗 听说你会吹牛逼\r\n",
      "No. 80568 label 1 = 0 lable 2 = 1 text pair: 孙红雷的电视剧 孙红梅的电视剧\r\n",
      "No. 81221 label 1 = 0 lable 2 = 1 text pair: 晨曦名字的含义是什么 晨旭名字的含义是什么\r\n",
      "No. 82043 label 1 = 0 lable 2 = 1 text pair: 如何报考 mba 如何报考mpa\r\n",
      "No. 84519 label 1 = 0 lable 2 = 1 text pair: 什么有助于睡眠 十么有助于睡眠\r\n",
      "No. 90811 label 1 = 0 lable 2 = 1 text pair: 你智力好吗 你智力好\r\n",
      "No. 91684 label 1 = 0 lable 2 = 1 text pair: 成语接龙吗 成语接龙啊\r\n",
      "No. 95977 label 1 = 0 lable 2 = 1 text pair: 0÷0等于几 0除以0等于多少\r\n",
      "No. 96223 label 1 = 0 lable 2 = 1 text pair: 会不会学猫叫你告诉我 会不会学猫叫我告诉你\r\n",
      "No. 99339 label 1 = 0 lable 2 = 1 text pair: 明宣宗之后是哪个皇帝 明宪宗之后是哪个皇帝\r\n",
      "No. 99738 label 1 = 0 lable 2 = 1 text pair: 你有爱人吗 你有爱人啊\r\n",
      "Total number of differences is 89\r\n",
      "Two prediction's similarity is 99.91\r\n",
      "不同的预测值共有： 89 组\r\n"
     ]
    }
   ],
   "source": [
    "#读取test和两个预测值\n",
    "test_text_list_1, test_text_list_2 = read_candidates_test(\"work/test_B_1118.tsv\")\n",
    "test_predict_label_list_1 = read_labels(\"predict_results/ccf_qianyan_qm_result_B.csv\")\n",
    "test_predict_label_list_2 = read_labels(\"predict_results/ms_ccf_qianyan_qm_result_B.csv\")\n",
    "print(len(test_text_list_1))\n",
    "print(len(test_text_list_2))\n",
    "print(len(test_predict_label_list_1))\n",
    "print(len(test_predict_label_list_2))\n",
    "\n",
    "# 对比两组预测值，并输出不同的预测结果\n",
    "def compre_predict_results(text_list1, text_list2, candidates_1, candidates_2):\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    label1 = []\n",
    "    label2 = []\n",
    "    j = 0\n",
    "    for i in range (0,len(candidates_1)): \n",
    "        if candidates_1[i] != candidates_2[i]:\n",
    "            print(\"No. \" + str(i) + \" label 1 = \" + str(candidates_1[i]) + \" lable 2 = \" + str(candidates_2[i]) + \" text pair: \" + text_list1[i] + \" \" + text_list2[i])\n",
    "            list1.append(text_list1[i])\n",
    "            list2.append(text_list2[i])\n",
    "            label1.append(candidates_1[i])\n",
    "            label2.append(candidates_2[i])\n",
    "            j = j + 1\n",
    "    print(\"Total number of differences is \" + str(j))\n",
    "    print(\"Two prediction's similarity is %.2f\"%((len(candidates_2) - j)/len(candidates_2)*100))\n",
    "    return list1, list2, label1, label2\n",
    "\n",
    "text_list1, text_list2, correct_text_list1, predict_text_list1= compre_predict_results(test_text_list_1, test_text_list_2, test_predict_label_list_1, test_predict_label_list_2)\n",
    "print(\"不同的预测值共有： \" + str(len(predict_text_list1)) + \" 组\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
